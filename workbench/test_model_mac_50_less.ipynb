{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch_geometric.data import Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import copy\n",
    "import random\n",
    "import multiprocessing as mp\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable CUDA debugging\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.set_device(1)  # Use GPU 1\n",
    "\n",
    "# # Verify that the correct GPU is being used\n",
    "# if torch.cuda.is_available():\n",
    "#     current_device = torch.cuda.current_device()\n",
    "#     print(f\"Using GPU: {torch.cuda.get_device_name(current_device)}\")\n",
    "#     print(f\"Device ID: {current_device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load node features\n",
    "s_df = pd.read_csv('/data/servilla/DT_HGNN/Model/Nodes/minus_50_train_substrate_node_dataset.csv', index_col=0)  # Substrates CSV file\n",
    "proteins_df = pd.read_csv('/data/servilla/DT_HGNN/Model/Nodes/minus_50_train_protein_node_dataset.csv', index_col=0)  # Combined proteins CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load edges (USE ONLY IF NEED TO RELOAD EDGES, OTHERWISE LOAD COMBINED EDGES FROM FILES)\n",
    "# tp_s_df = pd.read_csv('/data/servilla/DT_HGNN/Model/Edges/removed_50_train_tp_s.csv', index_col=0)\n",
    "ppi_df = pd.read_csv('/data/servilla/DT_HGNN/Model/Edges/Old_Edges/ppi_edges_6663523.csv', index_col=0)\n",
    "ssi_df = pd.read_csv('/data/servilla/DT_HGNN/Model/Edges/Old_Edges/ssi_edges_2177.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect and clean the data\n",
    "def inspect_and_clean(df):\n",
    "    non_numeric_columns = df.select_dtypes(exclude=[np.number]).columns\n",
    "    print(f\"Non-numeric columns: {non_numeric_columns}\")\n",
    "    if len(non_numeric_columns) > 0:\n",
    "        df[non_numeric_columns] = df[non_numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "    df = df.fillna(0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numeric columns: Index([], dtype='object')\n",
      "Non-numeric columns: Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "s_df = inspect_and_clean(s_df)\n",
    "proteins_df = inspect_and_clean(proteins_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert features to numpy arrays\n",
    "s_features = s_df.values\n",
    "p_features = proteins_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_features shape: (153, 1536)\n",
      "p_features shape: (237147, 2048)\n"
     ]
    }
   ],
   "source": [
    "# Check shapes to ensure correct dimensions\n",
    "print(f\"s_features shape: {s_features.shape}\")  # Expected (183, 1536), no KD (212, 768)\n",
    "print(f\"p_features shape: {p_features.shape}\")  # Expected (some number, 2048), no KD (571609, 1280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use GPU 1\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features (normalizes columns to have mean 0 and variance 1)\n",
    "s_features = (s_features - np.mean(s_features, axis=0)) / np.std(s_features, axis=0)\n",
    "p_features = (p_features - np.mean(p_features, axis=0)) / np.std(p_features, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformation layers, changes the number of features 1536 -> 2048\n",
    "# for substrates and 2048 -> 2048 for proteins. The transform_p layer is useful \n",
    "# for transforming the feature representation within the same dimensional space,\n",
    "#  y = Wx + b.\n",
    "\n",
    "device = torch.device('cpu')  # Temporarily switch to CPU\n",
    "\n",
    "\n",
    "transform_s = Linear(1536, 2048).to(device) # Change depending on the number of features\n",
    "transform_p = Linear(2048, 2048).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformations in batches, this can be useful when dealing with large \n",
    "# datasets that may not fit into memory or GPU all at once. \n",
    "def transform_in_batches(features, transform_layer, batch_size=10000):\n",
    "    num_samples = features.shape[0]\n",
    "    print(f\"Number of samples: {num_samples}\")\n",
    "    transformed_features = []\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        batch = features[i:i + batch_size]\n",
    "        batch_tensor = torch.tensor(batch, dtype=torch.float).to(device)\n",
    "        transformed_batch = transform_layer(batch_tensor)\n",
    "        transformed_features.append(transformed_batch.detach().cpu().numpy())  # Use detach() before numpy()\n",
    "    return np.vstack(transformed_features) # Stack arrays in sequence vertically (row wise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 153\n",
      "Number of samples: 237147\n"
     ]
    }
   ],
   "source": [
    "s_features_transformed = transform_in_batches(s_features, transform_s)\n",
    "p_features_transformed = transform_in_batches(p_features, transform_p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to tensors\n",
    "s_features_tensor = torch.tensor(s_features_transformed, dtype=torch.float).to(device)\n",
    "p_features_tensor = torch.tensor(p_features_transformed, dtype=torch.float).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine features, vertically stacks features (dim=0) to create a single tensor\n",
    "all_features = torch.cat([p_features_tensor, s_features_tensor], dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_ids = set(proteins_df.index)\n",
    "substrate_ids = set(s_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_cores = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_negative_edges_chunk(chunk, possible_sources, possible_targets, existing_edges):\n",
    "#     negative_edges = []\n",
    "#     for _ in chunk:\n",
    "#         while True:\n",
    "#             source = random.choice(possible_sources)\n",
    "#             target = random.choice(possible_targets)\n",
    "#             if (source, target) not in existing_edges and (target, source) not in existing_edges:\n",
    "#                 negative_edges.append((source, target))\n",
    "#                 break\n",
    "#     return negative_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_negative_edges(df, possible_sources, possible_targets, num_cores, chunk_size=10000):\n",
    "#     existing_edges = set(zip(df['source'], df['target']))\n",
    "\n",
    "#     # Create chunks\n",
    "#     chunks = [range(i, min(i + chunk_size, len(df))) for i in range(0, len(df), chunk_size)]\n",
    "    \n",
    "#     with mp.Pool(num_cores) as pool:\n",
    "#         results = pool.starmap(\n",
    "#             generate_negative_edges_chunk,\n",
    "#             [(chunk, possible_sources, possible_targets, existing_edges) for chunk in chunks]\n",
    "#         )\n",
    "\n",
    "#     # Flatten the list of lists\n",
    "#     negative_edges = [edge for sublist in results for edge in sublist]\n",
    "#     negative_df = pd.DataFrame(negative_edges, columns=['source', 'target'])\n",
    "#     return negative_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate negative edges for PPI\n",
    "# ppi_neg_df = generate_negative_edges(ppi_df, protein_ids, protein_ids, num_cores)\n",
    "# ppi_neg_df.to_csv('/data/servilla/DT_HGNN/Edges/negative_ppi_edges.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate negative edges for SSI\n",
    "# ssi_neg_df = generate_negative_edges(ssi_df, substrate_ids, substrate_ids, num_cores)\n",
    "# ssi_neg_df.to_csv('/data/servilla/DT_HGNN/Edges/negative_ssi_edges.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate negative edges for TP_S\n",
    "# tp_s_neg_df = generate_negative_edges(tp_s_df, protein_ids, substrate_ids, num_cores)\n",
    "# tp_s_neg_df.to_csv('/data/servilla/DT_HGNN/Edges/negative_tp_s_edges.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load negative edges\n",
    "# ppi_neg_df = pd.read_csv('/data/servilla/DT_HGNN/Model/Edges/Negative_Edges/negative_ppi_edges.csv', index_col=0)\n",
    "# ssi_neg_df = pd.read_csv('/data/servilla/DT_HGNN/Model/Edges/Negative_Edges/negative_ssi_edges.csv', index_col=0)\n",
    "# tp_s_neg_df = pd.read_csv('/data/servilla/DT_HGNN/Model/Edges/Negative_Edges/negative_tp_s_edges.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def concatenate_edges(pos_df, neg_df):\n",
    "#     # Add a label column to indicate positive (1) or negative (0) edges\n",
    "#     pos_df['label'] = 1\n",
    "#     neg_df['label'] = 0\n",
    "\n",
    "#     # Concatenate the positive and negative edges\n",
    "#     combined_df = pd.concat([pos_df, neg_df], ignore_index=True)\n",
    "#     return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Concatenate edges for each type\n",
    "# ppi_combined_df = concatenate_edges(ppi_df, ppi_neg_df)\n",
    "# ssi_combined_df = concatenate_edges(ssi_df, ssi_neg_df)\n",
    "# tp_s_combined_df = concatenate_edges(tp_s_df, tp_s_neg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the combined dataframes to CSV files\n",
    "# ppi_combined_df.to_csv('/data/servilla/DT_HGNN/Edges/combined_ppi_edges.csv', index=False)\n",
    "# ssi_combined_df.to_csv('/data/servilla/DT_HGNN/Edges/combined_ssi_edges.csv', index=False)\n",
    "# tp_s_combined_df.to_csv('/data/servilla/DT_HGNN/Edges/combined_tp_s_edges.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the combined edges\n",
    "ppi_combined_df = pd.read_csv('/data/servilla/DT_HGNN//Model/Edges/combined_ppi_edges.csv')\n",
    "ssi_combined_df = pd.read_csv('/data/servilla/DT_HGNN//Model/Edges/combined_ssi_edges.csv')\n",
    "tp_s_train_df = pd.read_csv('/data/servilla/DT_HGNN/Model/Edges/combined_tp_s_edges_minus_50.csv')\n",
    "tp_s_test_df = pd.read_csv('/data/servilla/DT_HGNN/Model/Edges/add_50_test_tp_s.csv')  # Add this line to load the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_data(df, train_size=0.8, val_size=0.1, test_size=0.1):\n",
    "#     # Split into train and temp (80% train, 20% temp)\n",
    "#     train_df, temp_df = train_test_split(df, train_size=train_size, random_state=42)\n",
    "    \n",
    "#     # Calculate the size for validation and test splits\n",
    "#     val_test_ratio = val_size / (val_size + test_size)  # 50% of temp goes to validation and 50% to test\n",
    "\n",
    "#     # Split temp into validation and test (10% each)\n",
    "#     val_df, test_df = train_test_split(temp_df, train_size=val_test_ratio, random_state=42)\n",
    "    \n",
    "#     return train_df, val_df, test_df\n",
    "\n",
    "def split_train_val_data(df, train_size=0.9, val_size=0.1):\n",
    "    # Split into train and validation (90% train, 10% validation)\n",
    "    train_df, val_df = train_test_split(df, train_size=train_size, random_state=42)\n",
    "    \n",
    "    return train_df, val_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split data for each edge type\n",
    "# ppi_train_df, ppi_val_df, ppi_test_df = split_data(ppi_combined_df)\n",
    "# ssi_train_df, ssi_val_df, ssi_test_df = split_data(ssi_combined_df)\n",
    "# tp_s_train_df, tp_s_val_df, tp_s_test_df = split_data(tp_s_combined_df)\n",
    "\n",
    "# Split data for each edge type (without test, as test is predefined)\n",
    "ppi_train_df, ppi_val_df = split_train_val_data(ppi_combined_df)\n",
    "ssi_train_df, ssi_val_df = split_train_val_data(ssi_combined_df)\n",
    "tp_s_train_df, tp_s_val_df = split_train_val_data(tp_s_train_df)  # Use remaining train data for tp_s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>P55156</td>\n",
       "      <td>CHEBI:64615</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5981</th>\n",
       "      <td>P41300</td>\n",
       "      <td>CHEBI:17562</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8113</th>\n",
       "      <td>O09008</td>\n",
       "      <td>CHEBI:57762</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9224</th>\n",
       "      <td>Q97ZQ3</td>\n",
       "      <td>CHEBI:17368</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7134</th>\n",
       "      <td>P16291</td>\n",
       "      <td>CHEBI:17968</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11964</th>\n",
       "      <td>Q31L27</td>\n",
       "      <td>CHEBI:82897</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5191</th>\n",
       "      <td>Q7N6B7</td>\n",
       "      <td>CHEBI:16113</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>P57860</td>\n",
       "      <td>CHEBI:57834</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>P58354</td>\n",
       "      <td>CHEBI:58539</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7270</th>\n",
       "      <td>P24929</td>\n",
       "      <td>CHEBI:72959</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11894 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       source       target  label\n",
       "944    P55156  CHEBI:64615      1\n",
       "5981   P41300  CHEBI:17562      0\n",
       "8113   O09008  CHEBI:57762      0\n",
       "9224   Q97ZQ3  CHEBI:17368      0\n",
       "7134   P16291  CHEBI:17968      0\n",
       "...       ...          ...    ...\n",
       "11964  Q31L27  CHEBI:82897      0\n",
       "5191   Q7N6B7  CHEBI:16113      0\n",
       "5390   P57860  CHEBI:57834      0\n",
       "860    P58354  CHEBI:58539      1\n",
       "7270   P24929  CHEBI:72959      0\n",
       "\n",
       "[11894 rows x 3 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp_s_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
      "       ...\n",
      "       '1526', '1527', '1528', '1529', '1530', '1531', '1532', '1533', '1534',\n",
      "       '1535'],\n",
      "      dtype='object', length=1536)\n"
     ]
    }
   ],
   "source": [
    "# Check the columns of the substrate DataFrame (s_df)\n",
    "print(s_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     0         1         2         3         4         5  \\\n",
      "ChEBI ID                                                                   \n",
      "CHEBI:64837   0.615461  0.182076 -0.157183 -0.101461 -0.146697 -0.613957   \n",
      "CHEBI:58245   0.506397  0.400621  0.064001 -0.252644 -0.300555 -0.395209   \n",
      "CHEBI:57673   0.438195  0.195334  0.072717 -0.337968 -0.261332 -0.498841   \n",
      "CHEBI:58115   0.263242  0.269032  0.145805 -0.515321 -0.187021 -0.480719   \n",
      "CHEBI:456215  0.285829  0.556172  0.108747 -0.328224 -0.301407 -0.238618   \n",
      "\n",
      "                     6         7         8         9  ...      1526      1527  \\\n",
      "ChEBI ID                                              ...                       \n",
      "CHEBI:64837  -0.450166 -0.183678 -0.624151 -0.271550  ...  0.107556 -0.293493   \n",
      "CHEBI:58245  -0.638782 -0.311317 -1.088331 -0.142135  ...  0.350439 -0.128131   \n",
      "CHEBI:57673  -0.686211 -0.416058 -0.848771 -0.195883  ...  0.379729 -0.210616   \n",
      "CHEBI:58115  -0.610310 -0.469400 -0.936680 -0.147377  ...  0.176869 -0.017013   \n",
      "CHEBI:456215 -0.653561 -0.339903 -1.186929 -0.152060  ...  0.154020  0.076386   \n",
      "\n",
      "                  1528      1529      1530      1531      1532      1533  \\\n",
      "ChEBI ID                                                                   \n",
      "CHEBI:64837  -0.117009  0.133840 -0.734478 -0.017846 -0.497699  0.045008   \n",
      "CHEBI:58245  -0.003493 -0.408475 -0.426437 -0.205415 -0.840490 -0.568522   \n",
      "CHEBI:57673   0.100455 -0.483885 -0.470335 -0.155050 -0.884785 -0.499907   \n",
      "CHEBI:58115   0.130641 -0.251108 -0.505997  0.049098 -0.935488 -0.402900   \n",
      "CHEBI:456215  0.300563 -0.133595 -0.276026 -0.035614 -0.997162 -0.386328   \n",
      "\n",
      "                  1534      1535  \n",
      "ChEBI ID                          \n",
      "CHEBI:64837  -0.386243 -0.474938  \n",
      "CHEBI:58245  -0.887295 -0.210402  \n",
      "CHEBI:57673  -0.748851 -0.243686  \n",
      "CHEBI:58115  -1.060279 -0.297512  \n",
      "CHEBI:456215 -0.917835 -0.385634  \n",
      "\n",
      "[5 rows x 1536 columns]\n",
      "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
      "       ...\n",
      "       '1526', '1527', '1528', '1529', '1530', '1531', '1532', '1533', '1534',\n",
      "       '1535'],\n",
      "      dtype='object', length=1536)\n"
     ]
    }
   ],
   "source": [
    "# Check the first few rows of s_df to understand its structure\n",
    "print(s_df.head())\n",
    "\n",
    "# Optionally, you can print the first few column names and their content\n",
    "print(s_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining NaNs in source_mapped: 0, Remaining NaNs in target_mapped: 0\n"
     ]
    }
   ],
   "source": [
    "# Access the ChEBI IDs from the index of s_df\n",
    "complete_substrate_list = list(s_df.index) + list(missing_substrates)  # Combine the existing substrates with missing ones\n",
    "\n",
    "# Generate the substrate mapping\n",
    "substrate_mapping = {substrate: i for i, substrate in enumerate(complete_substrate_list)}\n",
    "\n",
    "# Apply the updated mapping\n",
    "tp_s_train_df['source_mapped'] = tp_s_train_df['source'].map(protein_mapping)\n",
    "tp_s_train_df['target_mapped'] = tp_s_train_df['target'].map(substrate_mapping)\n",
    "\n",
    "# Check for remaining NaNs after mapping\n",
    "nan_sources = tp_s_train_df['source_mapped'].isna().sum()\n",
    "nan_targets = tp_s_train_df['target_mapped'].isna().sum()\n",
    "\n",
    "print(f\"Remaining NaNs in source_mapped: {nan_sources}, Remaining NaNs in target_mapped: {nan_targets}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of missing substrate IDs: []\n",
      "Total missing substrates: 0\n"
     ]
    }
   ],
   "source": [
    "# Print out the first few missing substrates to understand what's going wrong\n",
    "missing_substrates = tp_s_train_df[tp_s_train_df['target_mapped'].isna()]['target'].unique()\n",
    "print(f\"Sample of missing substrate IDs: {missing_substrates[:10]}\")\n",
    "print(f\"Total missing substrates: {len(missing_substrates)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Substrates missing in s_df: []\n"
     ]
    }
   ],
   "source": [
    "# Check if the missing substrates are present in the s_df index\n",
    "missing_in_s_df = [substrate for substrate in missing_substrates if substrate not in s_df.index]\n",
    "print(f\"Substrates missing in s_df: {missing_in_s_df}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original df size: (11894, 5)\n",
      "NaNs after mapping - Source: 0, Target: 0\n",
      "      source       target  label  source_mapped  target_mapped\n",
      "944   P55156  CHEBI:64615      1          17197             75\n",
      "5981  P41300  CHEBI:17562      0         100093             94\n",
      "8113  O09008  CHEBI:57762      0           2796            114\n",
      "9224  Q97ZQ3  CHEBI:17368      0         149714            144\n",
      "7134  P16291  CHEBI:17968      0          53203             44\n",
      "Mapped df size after dropping NaNs: (11894, 5)\n"
     ]
    }
   ],
   "source": [
    "def apply_correct_mapping(df, source_mapping, target_mapping):\n",
    "    # Before mapping, check the size of the dataframe\n",
    "    print(f\"Original df size: {df.shape}\")\n",
    "    \n",
    "    # Apply mapping\n",
    "    df['source_mapped'] = df['source'].map(source_mapping)\n",
    "    df['target_mapped'] = df['target'].map(target_mapping)\n",
    "    \n",
    "    # Print out any NaNs in the new columns\n",
    "    print(f\"NaNs after mapping - Source: {df['source_mapped'].isna().sum()}, Target: {df['target_mapped'].isna().sum()}\")\n",
    "\n",
    "    # Check the first few rows to verify mappings\n",
    "    print(df.head())\n",
    "\n",
    "    # Drop NaN values\n",
    "    df.dropna(subset=['source_mapped', 'target_mapped'], inplace=True)\n",
    "    \n",
    "    # Check the size of the dataframe after dropping NaNs\n",
    "    print(f\"Mapped df size after dropping NaNs: {df.shape}\")\n",
    "    \n",
    "    # Return the mapped dataframe with renamed columns\n",
    "    return df[['source_mapped', 'target_mapped', 'label']].rename(columns={'source_mapped': 'source', 'target_mapped': 'target'})\n",
    "\n",
    "# Apply the refined mapping function to the training set\n",
    "tp_s_train_df = apply_correct_mapping(tp_s_train_df, protein_mapping, substrate_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate mappings\n",
    "protein_mapping = {node_id: i for i, node_id in enumerate(proteins_df.index)}\n",
    "substrate_mapping = {node_id: i for i, node_id in enumerate(s_df.index)}\n",
    "\n",
    "# Helper function to apply the correct mapping\n",
    "def apply_correct_mapping(df, source_mapping, target_mapping):\n",
    "    df['source'] = df['source'].map(source_mapping)\n",
    "    df['target'] = df['target'].map(target_mapping)\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "# # Apply the correct mappings\n",
    "# tp_s_train_df = apply_correct_mapping(tp_s_train_df, protein_mapping, substrate_mapping)\n",
    "# tp_s_val_df = apply_correct_mapping(tp_s_val_df, protein_mapping, substrate_mapping)\n",
    "# tp_s_test_df = apply_correct_mapping(tp_s_test_df, protein_mapping, substrate_mapping)\n",
    "\n",
    "# ppi_train_df = apply_correct_mapping(ppi_train_df, protein_mapping, protein_mapping)\n",
    "# ppi_val_df = apply_correct_mapping(ppi_val_df, protein_mapping, protein_mapping)\n",
    "# ppi_test_df = apply_correct_mapping(ppi_test_df, protein_mapping, protein_mapping)\n",
    "\n",
    "# ssi_train_df = apply_correct_mapping(ssi_train_df, substrate_mapping, substrate_mapping)\n",
    "# ssi_val_df = apply_correct_mapping(ssi_val_df, substrate_mapping, substrate_mapping)\n",
    "# ssi_test_df = apply_correct_mapping(ssi_test_df, substrate_mapping, substrate_mapping)\n",
    "\n",
    "# Apply the correct mappings\n",
    "tp_s_train_df = apply_correct_mapping(tp_s_train_df, protein_mapping, substrate_mapping)\n",
    "tp_s_val_df = apply_correct_mapping(tp_s_val_df, protein_mapping, substrate_mapping)\n",
    "# # The test set mappings have already been applied during selection\n",
    "# tp_s_test_df = apply_correct_mapping(tp_s_train_df, protein_mapping, substrate_mapping)\n",
    "\n",
    "ppi_train_df = apply_correct_mapping(ppi_train_df, protein_mapping, protein_mapping)\n",
    "ppi_val_df = apply_correct_mapping(ppi_val_df, protein_mapping, protein_mapping)\n",
    "\n",
    "ssi_train_df = apply_correct_mapping(ssi_train_df, substrate_mapping, substrate_mapping)\n",
    "ssi_val_df = apply_correct_mapping(ssi_val_df, substrate_mapping, substrate_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protein mapping: [('A0A061ACU2', 0), ('A0A061AE05', 1), ('A0A061I403', 2), ('A0A072ULZ1', 3), ('A0A072VDF2', 4), ('A0A075F7E9', 5), ('A0A075QQ08', 6), ('A0A087WPF7', 7), ('A0A088MLT8', 8), ('A0A089QRB9', 9)]\n",
      "Substrate mapping: [('CHEBI:64837', 0), ('CHEBI:58245', 1), ('CHEBI:57673', 2), ('CHEBI:58115', 3), ('CHEBI:456215', 4), ('CHEBI:72999', 5), ('CHEBI:83228', 6), ('CHEBI:58950', 7), ('CHEBI:144584', 8), ('CHEBI:144582', 9)]\n",
      "Unique proteins in tp_s_train_df['source']: [ 17197 100093   2796 149714  53203  43274    473 161953 193620 141444]\n",
      "Unique substrates in tp_s_train_df['target']: [ 75  94 114 144  44  82 130  92  31  10]\n",
      "Missing proteins: 11894, Missing substrates: 11894\n"
     ]
    }
   ],
   "source": [
    "# Check the protein and substrate mappings\n",
    "print(f\"Protein mapping: {list(protein_mapping.items())[:10]}\")  # Print the first 10 items\n",
    "print(f\"Substrate mapping: {list(substrate_mapping.items())[:10]}\")  # Print the first 10 items\n",
    "\n",
    "# Check the unique values in the 'source' and 'target' columns of tp_s_train_df\n",
    "print(f\"Unique proteins in tp_s_train_df['source']: {tp_s_train_df['source'].unique()[:10]}\")\n",
    "print(f\"Unique substrates in tp_s_train_df['target']: {tp_s_train_df['target'].unique()[:10]}\")\n",
    "\n",
    "# Check if there are any missing nodes in the mappings\n",
    "missing_proteins = tp_s_train_df[~tp_s_train_df['source'].isin(protein_mapping.keys())]\n",
    "missing_substrates = tp_s_train_df[~tp_s_train_df['target'].isin(substrate_mapping.keys())]\n",
    "\n",
    "print(f\"Missing proteins: {len(missing_proteins)}, Missing substrates: {len(missing_substrates)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing proteins: 11894\n",
      "Number of missing substrates: 11894\n",
      "Sample missing proteins:\n",
      "[ 17197 100093   2796 149714  53203  43274    473 161953 193620 141444]\n",
      "Sample missing substrates:\n",
      "[ 75  94 114 144  44  82 130  92  31  10]\n"
     ]
    }
   ],
   "source": [
    "# Check if the 'source' proteins are in the protein mapping\n",
    "missing_proteins = tp_s_train_df[~tp_s_train_df['source'].isin(protein_mapping.keys())]\n",
    "print(f\"Number of missing proteins: {len(missing_proteins)}\")\n",
    "\n",
    "# Check if the 'target' substrates are in the substrate mapping\n",
    "missing_substrates = tp_s_train_df[~tp_s_train_df['target'].isin(substrate_mapping.keys())]\n",
    "print(f\"Number of missing substrates: {len(missing_substrates)}\")\n",
    "\n",
    "# Optionally, print some of the missing values for further inspection\n",
    "print(\"Sample missing proteins:\")\n",
    "print(missing_proteins['source'].unique()[:10])\n",
    "\n",
    "print(\"Sample missing substrates:\")\n",
    "print(missing_substrates['target'].unique()[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>17197</td>\n",
       "      <td>75</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5981</th>\n",
       "      <td>100093</td>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8113</th>\n",
       "      <td>2796</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9224</th>\n",
       "      <td>149714</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7134</th>\n",
       "      <td>53203</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11964</th>\n",
       "      <td>200663</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5191</th>\n",
       "      <td>136281</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>101992</td>\n",
       "      <td>134</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>17540</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7270</th>\n",
       "      <td>98789</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11894 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       source  target  label\n",
       "944     17197      75      1\n",
       "5981   100093      94      0\n",
       "8113     2796     114      0\n",
       "9224   149714     144      0\n",
       "7134    53203      44      0\n",
       "...       ...     ...    ...\n",
       "11964  200663      79      0\n",
       "5191   136281      68      0\n",
       "5390   101992     134      0\n",
       "860     17540      62      1\n",
       "7270    98789      40      0\n",
       "\n",
       "[11894 rows x 3 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp_s_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8414</th>\n",
       "      <td>7731</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4816</th>\n",
       "      <td>162104</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6149</th>\n",
       "      <td>135519</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1355</th>\n",
       "      <td>25592</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4464</th>\n",
       "      <td>182814</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4676</th>\n",
       "      <td>187964</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208</th>\n",
       "      <td>59015</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2057</th>\n",
       "      <td>195614</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>14897</td>\n",
       "      <td>107</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>14153</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1322 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      source  target  label\n",
       "8414    7731      74      0\n",
       "4816  162104       5      0\n",
       "6149  135519      62      0\n",
       "1355   25592      31      1\n",
       "4464  182814      11      0\n",
       "...      ...     ...    ...\n",
       "4676  187964      41      0\n",
       "1208   59015      13      1\n",
       "2057  195614      13      1\n",
       "424    14897     107      1\n",
       "554    14153      13      1\n",
       "\n",
       "[1322 rows x 3 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp_s_val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing substrate IDs: [ 75  94 114 144  44  82 130  92  31  10]\n",
      "Total missing substrates: 153\n",
      "Remaining NaNs in source_mapped: 11894, Remaining NaNs in target_mapped: 11894\n"
     ]
    }
   ],
   "source": [
    "# Ensure that mapping creates the 'target_mapped' column within the function\n",
    "tp_s_train_df['source_mapped'] = tp_s_train_df['source'].map(protein_mapping)\n",
    "tp_s_train_df['target_mapped'] = tp_s_train_df['target'].map(substrate_mapping)\n",
    "\n",
    "# Now, check for missing mappings after the columns are created\n",
    "missing_substrates = tp_s_train_df[tp_s_train_df['target_mapped'].isna()]['target'].unique()\n",
    "print(f\"Missing substrate IDs: {missing_substrates[:10]}\")  # Print a sample of missing IDs\n",
    "print(f\"Total missing substrates: {len(missing_substrates)}\")\n",
    "\n",
    "# Check remaining NaNs\n",
    "nan_sources = tp_s_train_df['source_mapped'].isna().sum()\n",
    "nan_targets = tp_s_train_df['target_mapped'].isna().sum()\n",
    "\n",
    "print(f\"Remaining NaNs in source_mapped: {nan_sources}, Remaining NaNs in target_mapped: {nan_targets}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8414</th>\n",
       "      <td>7731</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4816</th>\n",
       "      <td>162104</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6149</th>\n",
       "      <td>135519</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1355</th>\n",
       "      <td>25592</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4464</th>\n",
       "      <td>182814</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4676</th>\n",
       "      <td>187964</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208</th>\n",
       "      <td>59015</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2057</th>\n",
       "      <td>195614</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>14897</td>\n",
       "      <td>107</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>14153</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1322 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      source  target  label\n",
       "8414    7731      74      0\n",
       "4816  162104       5      0\n",
       "6149  135519      62      0\n",
       "1355   25592      31      1\n",
       "4464  182814      11      0\n",
       "...      ...     ...    ...\n",
       "4676  187964      41      0\n",
       "1208   59015      13      1\n",
       "2057  195614      13      1\n",
       "424    14897     107      1\n",
       "554    14153      13      1\n",
       "\n",
       "[1322 rows x 3 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp_s_val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>label</th>\n",
       "      <th>source_mapped</th>\n",
       "      <th>target_mapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [source, target, label, source_mapped, target_mapped]\n",
       "Index: []"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp_s_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test edge tensor created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Apply the correct mapping to the test set\n",
    "tp_s_test_df['source_mapped'] = tp_s_test_df['source'].map(protein_mapping)\n",
    "tp_s_test_df['target_mapped'] = tp_s_test_df['target'].map(substrate_mapping)\n",
    "\n",
    "# Drop any rows where the mapping resulted in NaNs\n",
    "tp_s_test_df.dropna(subset=['source_mapped', 'target_mapped'], inplace=True)\n",
    "\n",
    "# Convert the mapped columns to integers\n",
    "tp_s_test_df['source_mapped'] = tp_s_test_df['source_mapped'].astype(int)\n",
    "tp_s_test_df['target_mapped'] = tp_s_test_df['target_mapped'].astype(int)\n",
    "\n",
    "# Create edge index tensors\n",
    "test_edges_tp_s = torch.tensor(tp_s_test_df[['source_mapped', 'target_mapped']].values.T, dtype=torch.long)\n",
    "\n",
    "print(\"Test edge tensor created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m train_edges_tp_s \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(tp_s_train_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mT, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m     30\u001b[0m val_edges_tp_s \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(tp_s_val_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mT, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m---> 31\u001b[0m test_edges_tp_s \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtp_s_test_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msource\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtarget\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m train_edges_ppi \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(ppi_train_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mT, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m     34\u001b[0m val_edges_ppi \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(ppi_val_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mT, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool."
     ]
    }
   ],
   "source": [
    "# # Create edge index tensors\n",
    "# train_edges_tp_s = torch.tensor(tp_s_train_df[['source', 'target']].values.T, dtype=torch.long)\n",
    "# val_edges_tp_s = torch.tensor(tp_s_val_df[['source', 'target']].values.T, dtype=torch.long)\n",
    "# test_edges_tp_s = torch.tensor(tp_s_test_df[['source', 'target']].values.T, dtype=torch.long)\n",
    "\n",
    "# train_edges_ppi = torch.tensor(ppi_train_df[['source', 'target']].values.T, dtype=torch.long)\n",
    "# val_edges_ppi = torch.tensor(ppi_val_df[['source', 'target']].values.T, dtype=torch.long)\n",
    "# test_edges_ppi = torch.tensor(ppi_test_df[['source', 'target']].values.T, dtype=torch.long)\n",
    "\n",
    "# train_edges_ssi = torch.tensor(ssi_train_df[['source', 'target']].values.T, dtype=torch.long)\n",
    "# val_edges_ssi = torch.tensor(ssi_val_df[['source', 'target']].values.T, dtype=torch.long)\n",
    "# test_edges_ssi = torch.tensor(ssi_test_df[['source', 'target']].values.T, dtype=torch.long)\n",
    "\n",
    "# # Convert the labels to tensors\n",
    "# train_labels_tp_s = torch.tensor(tp_s_train_df['label'].values, dtype=torch.float)\n",
    "# val_labels_tp_s = torch.tensor(tp_s_val_df['label'].values, dtype=torch.float)\n",
    "# test_labels_tp_s = torch.tensor(tp_s_test_df['label'].values, dtype=torch.float)\n",
    "\n",
    "# train_labels_ppi = torch.tensor(ppi_train_df['label'].values, dtype=torch.float)\n",
    "# val_labels_ppi = torch.tensor(ppi_val_df['label'].values, dtype=torch.float)\n",
    "# test_labels_ppi = torch.tensor(ppi_test_df['label'].values, dtype=torch.float)\n",
    "\n",
    "# train_labels_ssi = torch.tensor(ssi_train_df['label'].values, dtype=torch.float)\n",
    "# val_labels_ssi = torch.tensor(ssi_val_df['label'].values, dtype=torch.float)\n",
    "# test_labels_ssi = torch.tensor(ssi_test_df['label'].values, dtype=torch.float)\n",
    "\n",
    "\n",
    "# Create edge index tensors for training, validation, and test\n",
    "train_edges_tp_s = torch.tensor(tp_s_train_df[['source', 'target']].values.T, dtype=torch.long)\n",
    "val_edges_tp_s = torch.tensor(tp_s_val_df[['source', 'target']].values.T, dtype=torch.long)\n",
    "test_edges_tp_s = torch.tensor(tp_s_test_df[['source', 'target']].values.T, dtype=torch.long)\n",
    "\n",
    "train_edges_ppi = torch.tensor(ppi_train_df[['source', 'target']].values.T, dtype=torch.long)\n",
    "val_edges_ppi = torch.tensor(ppi_val_df[['source', 'target']].values.T, dtype=torch.long)\n",
    "\n",
    "train_edges_ssi = torch.tensor(ssi_train_df[['source', 'target']].values.T, dtype=torch.long)\n",
    "val_edges_ssi = torch.tensor(ssi_val_df[['source', 'target']].values.T, dtype=torch.long)\n",
    "\n",
    "# Labels remain the same\n",
    "train_labels_tp_s = torch.tensor(tp_s_train_df['label'].values, dtype=torch.float)\n",
    "val_labels_tp_s = torch.tensor(tp_s_val_df['label'].values, dtype=torch.float)\n",
    "test_labels_tp_s = torch.tensor(tp_s_test_df['label'].values, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n"
     ]
    }
   ],
   "source": [
    "# Check for NaNs in node features\n",
    "print(torch.isnan(p_features_tensor).any())  # Should return False\n",
    "print(torch.isnan(s_features_tensor).any())  # Should return False\n",
    "\n",
    "# Check for NaNs in edge indices\n",
    "print(torch.isnan(train_edges_tp_s).any())  # Should return False\n",
    "print(torch.isnan(val_edges_tp_s).any())    # Should return False\n",
    "\n",
    "# Check for NaNs in labels\n",
    "print(torch.isnan(train_labels_tp_s).any())  # Should return False\n",
    "print(torch.isnan(val_labels_tp_s).any())    # Should return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output contains NaNs: False\n",
      "tensor([], grad_fn=<SqueezeBackward0>)\n",
      "Model output contains NaNs: False\n",
      "tensor([0.0591, 0.1030, 0.1702,  ..., 0.0687, 0.0838, 0.0791])\n",
      "Epoch 1, Train Loss: nan, Validation Loss: 0.7220, Val Accuracy: 0.1901, Precision: 0.1655, Recall: 0.9953, F1: 0.2838, AUC: 0.5560, LR: 0.0001\n",
      "Model output contains NaNs: False\n",
      "tensor([], grad_fn=<SqueezeBackward0>)\n",
      "Model output contains NaNs: False\n",
      "tensor([0.0591, 0.1030, 0.1702,  ..., 0.0687, 0.0838, 0.0791])\n",
      "Epoch 2, Train Loss: nan, Validation Loss: 0.7220, Val Accuracy: 0.1901, Precision: 0.1655, Recall: 0.9953, F1: 0.2838, AUC: 0.5560, LR: 0.0001\n",
      "Model output contains NaNs: False\n",
      "tensor([], grad_fn=<SqueezeBackward0>)\n",
      "Model output contains NaNs: False\n",
      "tensor([0.0591, 0.1030, 0.1702,  ..., 0.0687, 0.0838, 0.0791])\n",
      "Epoch 3, Train Loss: nan, Validation Loss: 0.7220, Val Accuracy: 0.1901, Precision: 0.1655, Recall: 0.9953, F1: 0.2838, AUC: 0.5560, LR: 0.0001\n",
      "Model output contains NaNs: False\n",
      "tensor([], grad_fn=<SqueezeBackward0>)\n",
      "Model output contains NaNs: False\n",
      "tensor([0.0591, 0.1030, 0.1702,  ..., 0.0687, 0.0838, 0.0791])\n",
      "Epoch 4, Train Loss: nan, Validation Loss: 0.7220, Val Accuracy: 0.1901, Precision: 0.1655, Recall: 0.9953, F1: 0.2838, AUC: 0.5560, LR: 0.0001\n",
      "Model output contains NaNs: False\n",
      "tensor([], grad_fn=<SqueezeBackward0>)\n",
      "Model output contains NaNs: False\n",
      "tensor([0.0591, 0.1030, 0.1702,  ..., 0.0687, 0.0838, 0.0791])\n",
      "Epoch 5, Train Loss: nan, Validation Loss: 0.7220, Val Accuracy: 0.1901, Precision: 0.1655, Recall: 0.9953, F1: 0.2838, AUC: 0.5560, LR: 0.0001\n",
      "Model output contains NaNs: False\n",
      "tensor([], grad_fn=<SqueezeBackward0>)\n",
      "Model output contains NaNs: False\n",
      "tensor([0.0591, 0.1030, 0.1702,  ..., 0.0687, 0.0838, 0.0791])\n",
      "Epoch 6, Train Loss: nan, Validation Loss: 0.7220, Val Accuracy: 0.1901, Precision: 0.1655, Recall: 0.9953, F1: 0.2838, AUC: 0.5560, LR: 0.0001\n",
      "Model output contains NaNs: False\n",
      "tensor([], grad_fn=<SqueezeBackward0>)\n",
      "Model output contains NaNs: False\n",
      "tensor([0.0591, 0.1030, 0.1702,  ..., 0.0687, 0.0838, 0.0791])\n",
      "Epoch 7, Train Loss: nan, Validation Loss: 0.7220, Val Accuracy: 0.1901, Precision: 0.1655, Recall: 0.9953, F1: 0.2838, AUC: 0.5560, LR: 1e-05\n",
      "Model output contains NaNs: False\n",
      "tensor([], grad_fn=<SqueezeBackward0>)\n",
      "Model output contains NaNs: False\n",
      "tensor([0.0591, 0.1030, 0.1702,  ..., 0.0687, 0.0838, 0.0791])\n",
      "Epoch 8, Train Loss: nan, Validation Loss: 0.7220, Val Accuracy: 0.1901, Precision: 0.1655, Recall: 0.9953, F1: 0.2838, AUC: 0.5560, LR: 1e-05\n",
      "Model output contains NaNs: False\n",
      "tensor([], grad_fn=<SqueezeBackward0>)\n",
      "Model output contains NaNs: False\n",
      "tensor([0.0591, 0.1030, 0.1702,  ..., 0.0687, 0.0838, 0.0791])\n",
      "Epoch 9, Train Loss: nan, Validation Loss: 0.7220, Val Accuracy: 0.1901, Precision: 0.1655, Recall: 0.9953, F1: 0.2838, AUC: 0.5560, LR: 1e-05\n",
      "Model output contains NaNs: False\n",
      "tensor([], grad_fn=<SqueezeBackward0>)\n",
      "Model output contains NaNs: False\n",
      "tensor([0.0591, 0.1030, 0.1702,  ..., 0.0687, 0.0838, 0.0791])\n",
      "Epoch 10, Train Loss: nan, Validation Loss: 0.7220, Val Accuracy: 0.1901, Precision: 0.1655, Recall: 0.9953, F1: 0.2838, AUC: 0.5560, LR: 1e-05\n",
      "Model output contains NaNs: False\n",
      "tensor([], grad_fn=<SqueezeBackward0>)\n",
      "Model output contains NaNs: False\n",
      "tensor([0.0591, 0.1030, 0.1702,  ..., 0.0687, 0.0838, 0.0791])\n",
      "Epoch 11, Train Loss: nan, Validation Loss: 0.7220, Val Accuracy: 0.1901, Precision: 0.1655, Recall: 0.9953, F1: 0.2838, AUC: 0.5560, LR: 1e-05\n",
      "Early stopping at epoch 11\n",
      "Model output contains NaNs: False\n",
      "tensor([])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miservilla/miniconda3/envs/MLG/lib/python3.8/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/home/miservilla/miniconda3/envs/MLG/lib/python3.8/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/miservilla/miniconda3/envs/MLG/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/miservilla/miniconda3/envs/MLG/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/miservilla/miniconda3/envs/MLG/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 178\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# Testing\u001b[39;00m\n\u001b[1;32m    177\u001b[0m test_loss, test_preds \u001b[38;5;241m=\u001b[39m test()\n\u001b[0;32m--> 178\u001b[0m test_accuracy, test_precision, test_recall, test_f1, test_auc \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_labels_tp_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_preds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Precision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_precision\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Recall: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_recall\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_f1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, AUC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_auc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[22], line 128\u001b[0m, in \u001b[0;36mcalculate_metrics\u001b[0;34m(labels, preds)\u001b[0m\n\u001b[1;32m    126\u001b[0m recall \u001b[38;5;241m=\u001b[39m recall_score(labels, preds_binary)\n\u001b[1;32m    127\u001b[0m f1 \u001b[38;5;241m=\u001b[39m f1_score(labels, preds_binary)\n\u001b[0;32m--> 128\u001b[0m auc \u001b[38;5;241m=\u001b[39m \u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m accuracy, precision, recall, f1, auc\n",
      "File \u001b[0;32m~/miniconda3/envs/MLG/lib/python3.8/site-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n\u001b[0;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    224\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/MLG/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:605\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) \\\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;124;03mfrom prediction scores.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;124;03marray([0.81..., 0.84... , 0.93..., 0.87..., 0.94...])\u001b[39;00m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    604\u001b[0m y_type \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 605\u001b[0m y_true \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    606\u001b[0m y_score \u001b[38;5;241m=\u001b[39m check_array(y_score, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    609\u001b[0m     y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m y_score\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m y_score\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    610\u001b[0m ):\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;66;03m# do not support partial ROC computation for multiclass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/MLG/lib/python3.8/site-packages/sklearn/utils/validation.py:967\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    965\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[0;32m--> 967\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    968\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    969\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    970\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[1;32m    971\u001b[0m         )\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    974\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "data = HeteroData()\n",
    "\n",
    "# Assign node features\n",
    "data['protein'].x = p_features_tensor\n",
    "data['substrate'].x = s_features_tensor\n",
    "\n",
    "# Assign training edges\n",
    "data['protein', 'interacts_with', 'substrate'].edge_index = train_edges_tp_s\n",
    "data['protein', 'interacts_with', 'protein'].edge_index = train_edges_ppi\n",
    "data['substrate', 'interacts_with', 'substrate'].edge_index = train_edges_ssi\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "class GCNLinkPredictor(nn.Module):\n",
    "    def __init__(self, protein_dim, substrate_dim, hidden_channels):\n",
    "        super(GCNLinkPredictor, self).__init__()\n",
    "        self.protein_conv1 = GCNConv(protein_dim, hidden_channels)\n",
    "        self.substrate_conv1 = GCNConv(substrate_dim, hidden_channels)\n",
    "        self.protein_conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.substrate_conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.link_predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_channels * 2, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels, 1)\n",
    "        )\n",
    "\n",
    "    def encode(self, x_dict, edge_index_dict):\n",
    "        z_protein = self.protein_conv1(x_dict['protein'], edge_index_dict[('protein', 'interacts_with', 'protein')])\n",
    "        z_substrate = self.substrate_conv1(x_dict['substrate'], edge_index_dict[('substrate', 'interacts_with', 'substrate')])\n",
    "        return z_protein, z_substrate\n",
    "\n",
    "    # def forward(self, x_dict, edge_index_dict, edges):\n",
    "    #     z_protein, z_substrate = self.encode(x_dict, edge_index_dict)\n",
    "    #     z_combined = torch.cat([z_protein[edges[0]], z_substrate[edges[1]]], dim=-1)\n",
    "    #     return self.link_predictor(z_combined).squeeze()\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, edges):\n",
    "        if edges.size(1) == 0:  # Check if the edges tensor is empty\n",
    "            print(\"Warning: Empty batch of edges!\")\n",
    "            return torch.tensor([], device=device)  # Return an empty tensor to handle this case\n",
    "        \n",
    "        z_protein, z_substrate = self.encode(x_dict, edge_index_dict)\n",
    "        z_combined = torch.cat([z_protein[edges[0]], z_substrate[edges[1]]], dim=-1)\n",
    "        output = self.link_predictor(z_combined).squeeze()\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = GCNLinkPredictor(protein_dim=2048, substrate_dim=2048, hidden_channels=64).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 10  # Number of epochs to wait before stopping if no improvement\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=4)\n",
    "\n",
    "\n",
    "# Assuming `data` contains x_dict and edge_index_dict\n",
    "x_dict = data.x_dict\n",
    "edge_index_dict = data.edge_index_dict\n",
    "\n",
    "\n",
    "# # Update the train, validate, and test functions to return predictions\n",
    "# def train(x_dict, edge_index_dict, train_edges_tp_s, train_labels_tp_s): \n",
    "#     model.train()\n",
    "#     optimizer.zero_grad()\n",
    "#     out = model(x_dict, edge_index_dict, train_edges_tp_s)\n",
    "#     loss = criterion(out, train_labels_tp_s)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     return loss.item(), out.detach()\n",
    "\n",
    "# def validate():\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         out = model(data.x_dict, data.edge_index_dict, val_edges_tp_s)\n",
    "#         loss = criterion(out, val_labels_tp_s)\n",
    "#     return loss.item(), out\n",
    "\n",
    "# def test():\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         out = model(data.x_dict, data.edge_index_dict, test_edges_tp_s)\n",
    "#         loss = criterion(out, test_labels_tp_s)\n",
    "#     return loss.item(), out\n",
    "\n",
    "# Update the train, validate, and test functions as needed\n",
    "def train(x_dict, edge_index_dict, train_edges_tp_s, train_labels_tp_s): \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(x_dict, edge_index_dict, train_edges_tp_s)\n",
    "    loss = criterion(out, train_labels_tp_s)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item(), out.detach()\n",
    "\n",
    "def validate():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x_dict, data.edge_index_dict, val_edges_tp_s)\n",
    "        loss = criterion(out, val_labels_tp_s)\n",
    "    return loss.item(), out\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x_dict, data.edge_index_dict, test_edges_tp_s)\n",
    "        loss = criterion(out, test_labels_tp_s)\n",
    "    return loss.item(), out\n",
    "\n",
    "# Calculate additional metrics\n",
    "def calculate_metrics(labels, preds):\n",
    "    preds = torch.sigmoid(preds).cpu().numpy()\n",
    "    preds_binary = (preds > 0.5).astype(int)\n",
    "    labels = labels.cpu().numpy()\n",
    "\n",
    "    accuracy = accuracy_score(labels, preds_binary)\n",
    "    precision = precision_score(labels, preds_binary)\n",
    "    recall = recall_score(labels, preds_binary)\n",
    "    f1 = f1_score(labels, preds_binary)\n",
    "    auc = roc_auc_score(labels, preds)\n",
    "\n",
    "    return accuracy, precision, recall, f1, auc\n",
    "\n",
    "# Modify the training loop to include metric calculation and visualization\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Training loop\n",
    "epochs = 800\n",
    "for epoch in range(epochs):\n",
    "    # Training step\n",
    "    train_loss, train_preds = train(x_dict, edge_index_dict, train_edges_tp_s, train_labels_tp_s)\n",
    "    # Validation step\n",
    "    val_loss, val_preds = validate()\n",
    "\n",
    "    # Store losses\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # Calculate validation metrics\n",
    "    accuracy, precision, recall, f1, auc = calculate_metrics(val_labels_tp_s, val_preds)\n",
    "    val_accuracies.append(accuracy)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, \"\n",
    "          f\"Val Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, \"\n",
    "          f\"F1: {f1:.4f}, AUC: {auc:.4f}, LR: {scheduler.get_last_lr()[0]}\")\n",
    "\n",
    "    # Step the LR scheduler\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Check for early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_without_improvement = 0\n",
    "        torch.save(model.state_dict(), '/data/servilla/DT_HGNN/data/Models_saves/best_model.pth')  # Save the best model\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "    \n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('/data/servilla/DT_HGNN/data/Models_saves/best_model.pth'))\n",
    "\n",
    "# Testing\n",
    "test_loss, test_preds = test()\n",
    "test_accuracy, test_precision, test_recall, test_f1, test_auc = calculate_metrics(test_labels_tp_s, test_preds)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}, AUC: {test_auc:.4f}\")\n",
    "\n",
    "# Step 4: Plot loss curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss Curves\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Step 5: Plot validation accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(val_accuracies, label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
