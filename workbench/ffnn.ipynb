{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch_geometric.data import Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import copy\n",
    "import random\n",
    "import multiprocessing as mp\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, precision_recall_curve, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch_geometric.utils import to_undirected\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from datetime import datetime\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load node data from csv\n",
    "s_df = pd.read_csv('/data/servilla/DT_HGNN/Model/Nodes/s_emb_full_183.csv', index_col=0)\n",
    "p_df = pd.read_csv('/data/servilla/DT_HGNN/Model/Nodes/Other/tp_only_12301.csv', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load edge data from csv\n",
    "tp_s_df = pd.read_csv('/data/servilla/DT_HGNN/Model/Edges/distributed_combined_tp_s_edges_13340.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q1CR30</td>\n",
       "      <td>CHEBI:30616</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q2A1I2</td>\n",
       "      <td>CHEBI:30616</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O67337</td>\n",
       "      <td>CHEBI:132124</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B1MPH4</td>\n",
       "      <td>CHEBI:132124</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q6HP89</td>\n",
       "      <td>CHEBI:30616</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26675</th>\n",
       "      <td>Q3JC24</td>\n",
       "      <td>CHEBI:456216</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26676</th>\n",
       "      <td>B2U7R1</td>\n",
       "      <td>CHEBI:30616</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26677</th>\n",
       "      <td>B2VIV5</td>\n",
       "      <td>CHEBI:15378</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26678</th>\n",
       "      <td>P0C323</td>\n",
       "      <td>CHEBI:132124</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26679</th>\n",
       "      <td>Q92G88</td>\n",
       "      <td>CHEBI:132124</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26680 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       source        target  label\n",
       "0      Q1CR30   CHEBI:30616      1\n",
       "1      Q2A1I2   CHEBI:30616      1\n",
       "2      O67337  CHEBI:132124      1\n",
       "3      B1MPH4  CHEBI:132124      1\n",
       "4      Q6HP89   CHEBI:30616      1\n",
       "...       ...           ...    ...\n",
       "26675  Q3JC24  CHEBI:456216      0\n",
       "26676  B2U7R1   CHEBI:30616      0\n",
       "26677  B2VIV5   CHEBI:15378      0\n",
       "26678  P0C323  CHEBI:132124      0\n",
       "26679  Q92G88  CHEBI:132124      0\n",
       "\n",
       "[26680 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp_s_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load device of available GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == 'cuda':\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q1CR30</td>\n",
       "      <td>CHEBI:30616</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q2A1I2</td>\n",
       "      <td>CHEBI:30616</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O67337</td>\n",
       "      <td>CHEBI:132124</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B1MPH4</td>\n",
       "      <td>CHEBI:132124</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q6HP89</td>\n",
       "      <td>CHEBI:30616</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26675</th>\n",
       "      <td>Q3JC24</td>\n",
       "      <td>CHEBI:456216</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26676</th>\n",
       "      <td>B2U7R1</td>\n",
       "      <td>CHEBI:30616</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26677</th>\n",
       "      <td>B2VIV5</td>\n",
       "      <td>CHEBI:15378</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26678</th>\n",
       "      <td>P0C323</td>\n",
       "      <td>CHEBI:132124</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26679</th>\n",
       "      <td>Q92G88</td>\n",
       "      <td>CHEBI:132124</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26680 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       source        target  label\n",
       "0      Q1CR30   CHEBI:30616      1\n",
       "1      Q2A1I2   CHEBI:30616      1\n",
       "2      O67337  CHEBI:132124      1\n",
       "3      B1MPH4  CHEBI:132124      1\n",
       "4      Q6HP89   CHEBI:30616      1\n",
       "...       ...           ...    ...\n",
       "26675  Q3JC24  CHEBI:456216      0\n",
       "26676  B2U7R1   CHEBI:30616      0\n",
       "26677  B2VIV5   CHEBI:15378      0\n",
       "26678  P0C323  CHEBI:132124      0\n",
       "26679  Q92G88  CHEBI:132124      0\n",
       "\n",
       "[26680 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp_s_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Definitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the current date and time for the file name and log\n",
    "def get_timestamp():\n",
    "    return datetime.now().strftime('%Y-%m-%d_%H-%M-%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect and clean the data, converts non-numeric columns to numeric and fills NaN values with 0\n",
    "def inspect_and_clean(df):\n",
    "    non_numeric_columns = df.select_dtypes(exclude=[np.number]).columns\n",
    "    print(f\"Non-numeric columns: {non_numeric_columns}\")\n",
    "    if len(non_numeric_columns) > 0:\n",
    "        df[non_numeric_columns] = df[non_numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "    df = df.fillna(0)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_correct_mapping(df, source_mapping, target_mapping):\n",
    "    # Apply the mappings to 'source' and 'target'\n",
    "    df['mapped_source'] = df['source'].map(source_mapping)\n",
    "    df['mapped_target'] = df['target'].map(target_mapping)\n",
    "\n",
    "    # Identify rows where mapping failed (NaN values)\n",
    "    unmapped_sources = df[df['mapped_source'].isna()]['source'].unique()\n",
    "    unmapped_targets = df[df['mapped_target'].isna()]['target'].unique()\n",
    "\n",
    "    # Log or print unmapped elements\n",
    "    if len(unmapped_sources) > 0:\n",
    "        print(f\"Unmapped sources: {unmapped_sources}\")\n",
    "    if len(unmapped_targets) > 0:\n",
    "        print(f\"Unmapped targets: {unmapped_targets}\")\n",
    "\n",
    "    # Remove rows where mapping failed (NaN values)\n",
    "    df.dropna(subset=['mapped_source', 'mapped_target'], inplace=True)\n",
    "\n",
    "    # Replace original 'source' and 'target' with mapped values and drop the extra columns\n",
    "    df['source'] = df['mapped_source']\n",
    "    df['target'] = df['mapped_target']\n",
    "    df.drop(columns=['mapped_source', 'mapped_target'], inplace=True)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to manipulate data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numeric columns: Index([], dtype='object')\n",
      "Non-numeric columns: Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "s_df = inspect_and_clean(s_df)\n",
    "p_df = inspect_and_clean(p_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert features to numpy arrays\n",
    "s_features = s_df.values\n",
    "p_features = p_df.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of s_features: (183, 1536)\n",
      "Shape of p_features: (12301, 2048)\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of the features\n",
    "print(f\"Shape of s_features: {s_features.shape}\")\n",
    "print(f\"Shape of p_features: {p_features.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate mappings\n",
    "protein_mapping = {node_id: i for i, node_id in enumerate(p_df.index)}\n",
    "substrate_mapping = {node_id: i for i, node_id in enumerate(s_df.index)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply mappings to the full edge DataFrames before splitting\n",
    "tp_s_df = apply_correct_mapping(tp_s_df, protein_mapping, substrate_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6048</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6389</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4380</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3165</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8328</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26675</th>\n",
       "      <td>7165</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26676</th>\n",
       "      <td>3351</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26677</th>\n",
       "      <td>3375</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26678</th>\n",
       "      <td>4590</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26679</th>\n",
       "      <td>9713</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26680 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       source  target  label\n",
       "0        6048       0      1\n",
       "1        6389       0      1\n",
       "2        4380      63      1\n",
       "3        3165      63      1\n",
       "4        8328       0      1\n",
       "...       ...     ...    ...\n",
       "26675    7165      18      0\n",
       "26676    3351       0      0\n",
       "26677    3375      19      0\n",
       "26678    4590      63      0\n",
       "26679    9713      63      0\n",
       "\n",
       "[26680 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp_s_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the features to tensors (first convert to numpy arrays)\n",
    "s_np = s_df.values\n",
    "p_np = p_df.values\n",
    "\n",
    "s_features_tensor = torch.tensor(s_np, dtype=torch.float).to(device)\n",
    "p_features_tensor = torch.tensor(p_np, dtype=torch.float).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0171, -0.0576, -0.0224,  ...,  0.3624, -0.1807,  0.1883],\n",
       "        [-0.0125, -0.0156, -0.0371,  ...,  0.4206, -0.2489,  0.1974],\n",
       "        [ 0.0028, -0.0836, -0.0325,  ...,  0.3974, -0.2052,  0.1842],\n",
       "        ...,\n",
       "        [ 0.0979, -0.0349,  0.0161,  ...,  0.4393, -0.1220,  0.1718],\n",
       "        [ 0.0939, -0.1774,  0.2202,  ...,  0.4017, -0.1780,  0.1699],\n",
       "        [ 0.0565, -0.0809,  0.1186,  ...,  0.3937, -0.1902,  0.1530]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_features_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4558,  0.2683, -0.3065,  ..., -0.3660, -0.8484, -0.2447],\n",
       "        [ 0.6155,  0.1821, -0.1572,  ...,  0.0450, -0.3862, -0.4749],\n",
       "        [ 0.5064,  0.4006,  0.0640,  ..., -0.5685, -0.8873, -0.2104],\n",
       "        ...,\n",
       "        [ 0.6212,  0.5304, -0.4603,  ..., -0.2120,  0.0052, -0.5082],\n",
       "        [-0.1077,  0.3367, -0.0385,  ...,  0.0214, -0.0697, -0.3905],\n",
       "        [ 0.4057, -0.2626, -0.5516,  ..., -0.1872, -0.2486, -0.3938]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_features_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Assuming tp_s_df['source'] and tp_s_df['target'] contain valid mapped indices\n",
    "\n",
    "# Convert the source and target columns to tensors\n",
    "protein_indices = torch.tensor(tp_s_df['source'].values, dtype=torch.long).to(p_features_tensor.device)\n",
    "substrate_indices = torch.tensor(tp_s_df['target'].values, dtype=torch.long).to(s_features_tensor.device)\n",
    "\n",
    "# Index the protein and substrate feature tensors\n",
    "p_features_tensor_mapped = p_features_tensor[protein_indices]\n",
    "s_features_tensor_mapped = s_features_tensor[substrate_indices]\n",
    "\n",
    "# Convert labels to tensor\n",
    "labels_tensor = torch.tensor(tp_s_df['label'].values, dtype=torch.float32).to(p_features_tensor.device)\n",
    "\n",
    "# # Create a TensorDataset\n",
    "# dataset = TensorDataset(p_features_tensor_mapped, s_features_tensor_mapped, labels_tensor)\n",
    "\n",
    "# # Split the dataset into training (60%), validation (20%), and test (20%) sets\n",
    "# train_val_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)  # 80% train+val, 20% test\n",
    "# train_data, val_data = train_test_split(train_val_data, test_size=0.25, random_state=42)  # 60% train, 20% val\n",
    "\n",
    "# # Create DataLoaders for training, validation, and test sets\n",
    "# train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "# val_loader = DataLoader(val_data, batch_size=32, shuffle=False)  # No shuffling for validation\n",
    "# test_loader = DataLoader(test_data, batch_size=32, shuffle=False)  # No shuffling for test set\n",
    "\n",
    "# # Check the first batch of data from the training loader\n",
    "# for batch in train_loader:\n",
    "#     p_features, s_features, labels = batch\n",
    "#     print(p_features)\n",
    "#     print(s_features)\n",
    "#     print(labels)\n",
    "#     break\n",
    "\n",
    "# Add protein and substrate indices to the TensorDataset\n",
    "dataset = TensorDataset(p_features_tensor_mapped, s_features_tensor_mapped, labels_tensor, protein_indices, substrate_indices)\n",
    "\n",
    "# Split the dataset into training (60%), validation (20%), and test (20%) sets\n",
    "train_val_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)  # 80% train+val, 20% test\n",
    "train_data, val_data = train_test_split(train_val_data, test_size=0.25, random_state=42)  # 60% train, 20% val\n",
    "\n",
    "# Create DataLoaders for training, validation, and test sets\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)  # No shuffling for validation\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)  # No shuffling for test set\n",
    "\n",
    "# # Check the first batch of data from the test loader to extract protein and substrate indices\n",
    "# for batch in test_loader:\n",
    "#     p_features, s_features, labels, protein_ids_batch, substrate_ids_batch = batch\n",
    "#     print(\"Protein Indices in Test Batch:\", protein_ids_batch)\n",
    "#     print(\"Substrate Indices in Test Batch:\", substrate_ids_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26680"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(protein_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# # Assuming tp_s_df['source'], tp_s_df['target'], and tp_s_df['label'] are valid\n",
    "\n",
    "# # Step 1: Split off 20% for a naive test set (this is the first step before any other splits)\n",
    "# tp_s_train_val_df, tp_s_test_df = train_test_split(tp_s_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Step 2: Prepare the naive test set (20% held out)\n",
    "# test_protein_indices = torch.tensor(tp_s_test_df['source'].values, dtype=torch.long).to(p_features_tensor.device)\n",
    "# test_substrate_indices = torch.tensor(tp_s_test_df['target'].values, dtype=torch.long).to(s_features_tensor.device)\n",
    "\n",
    "# test_p_features_tensor_mapped = p_features_tensor[test_protein_indices]\n",
    "# test_s_features_tensor_mapped = s_features_tensor[test_substrate_indices]\n",
    "\n",
    "# test_labels_tensor = torch.tensor(tp_s_test_df['label'].values, dtype=torch.float32).to(p_features_tensor.device)\n",
    "\n",
    "# # Create a TensorDataset and DataLoader for the naive test set\n",
    "# test_dataset = TensorDataset(test_p_features_tensor_mapped, test_s_features_tensor_mapped, test_labels_tensor)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# # Step 3: Now split the remaining 80% into training (60%) and validation (20%)\n",
    "# protein_indices_train_val = torch.tensor(tp_s_train_val_df['source'].values, dtype=torch.long).to(p_features_tensor.device)\n",
    "# substrate_indices_train_val = torch.tensor(tp_s_train_val_df['target'].values, dtype=torch.long).to(s_features_tensor.device)\n",
    "\n",
    "# p_features_tensor_mapped_train_val = p_features_tensor[protein_indices_train_val]\n",
    "# s_features_tensor_mapped_train_val = s_features_tensor[substrate_indices_train_val]\n",
    "# labels_tensor_train_val = torch.tensor(tp_s_train_val_df['label'].values, dtype=torch.float32).to(p_features_tensor.device)\n",
    "\n",
    "# # Create the TensorDataset for training and validation\n",
    "# train_val_dataset = TensorDataset(p_features_tensor_mapped_train_val, s_features_tensor_mapped_train_val, labels_tensor_train_val)\n",
    "\n",
    "# # Step 4: Split the remaining 80% into 60% training and 20% validation\n",
    "# train_data, val_data = train_test_split(train_val_dataset, test_size=0.25, random_state=42)  # 60% train, 20% val\n",
    "\n",
    "# # Step 5: Create DataLoaders for training and validation\n",
    "# train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "# val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# # Assuming tp_s_df has already been mapped (with 'source', 'target', and 'label' columns)\n",
    "\n",
    "# # Step 1: Shuffle the dataset to ensure even distribution\n",
    "# tp_s_df = tp_s_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# # Step 2: Calculate the split index for 20%\n",
    "# split_index = int(len(tp_s_df) * 0.8)\n",
    "\n",
    "# # Step 3: Split the dataset into 80% for training/validation and 20% for testing\n",
    "# train_val_df = tp_s_df[:split_index]\n",
    "# test_df = tp_s_df[split_index:]\n",
    "\n",
    "# # Step 4: Convert the train/validation and test sets into PyTorch tensors\n",
    "\n",
    "# # For training/validation set\n",
    "# train_protein_indices = torch.tensor(train_val_df['source'].values, dtype=torch.long).to(p_features_tensor.device)\n",
    "# train_substrate_indices = torch.tensor(train_val_df['target'].values, dtype=torch.long).to(s_features_tensor.device)\n",
    "\n",
    "# train_p_features_tensor_mapped = p_features_tensor[train_protein_indices]\n",
    "# train_s_features_tensor_mapped = s_features_tensor[train_substrate_indices]\n",
    "# train_labels_tensor = torch.tensor(train_val_df['label'].values, dtype=torch.float32).to(p_features_tensor.device)\n",
    "\n",
    "# # Create TensorDataset for training/validation\n",
    "# train_val_dataset = TensorDataset(train_p_features_tensor_mapped, train_s_features_tensor_mapped, train_labels_tensor)\n",
    "\n",
    "# # For test set\n",
    "# test_protein_indices = torch.tensor(test_df['source'].values, dtype=torch.long).to(p_features_tensor.device)\n",
    "# test_substrate_indices = torch.tensor(test_df['target'].values, dtype=torch.long).to(s_features_tensor.device)\n",
    "\n",
    "# test_p_features_tensor_mapped = p_features_tensor[test_protein_indices]\n",
    "# test_s_features_tensor_mapped = s_features_tensor[test_substrate_indices]\n",
    "# test_labels_tensor = torch.tensor(test_df['label'].values, dtype=torch.float32).to(p_features_tensor.device)\n",
    "\n",
    "# # Create TensorDataset for testing\n",
    "# test_dataset = TensorDataset(test_p_features_tensor_mapped, test_s_features_tensor_mapped, test_labels_tensor)\n",
    "\n",
    "# # Step 5: Create DataLoaders for training/validation and test sets\n",
    "# train_loader = DataLoader(train_val_dataset, batch_size=32, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NonGraphModel(nn.Module):\n",
    "    def __init__(self, protein_input_dim, substrate_input_dim, hidden_dim, output_dim, dropout_prob=0.5):\n",
    "        super(NonGraphModel, self).__init__()\n",
    "\n",
    "        # Transformation layers to match input dimensions\n",
    "        self.transform_p = nn.Linear(protein_input_dim, hidden_dim)  # Protein: 2048 -> 128\n",
    "        self.transform_s = nn.Linear(substrate_input_dim, hidden_dim)  # Substrate: 1536 -> 128\n",
    "\n",
    "        # Additional fully connected layers\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)  # Combine protein + substrate, then reduce dimension\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    def forward(self, p_features, s_features):\n",
    "        # Apply transformations to ensure both node types have the same dimensionality\n",
    "        p_transformed = self.transform_p(p_features)  # Protein features: 2048 -> 128\n",
    "        s_transformed = self.transform_s(s_features)  # Substrate features: 1536 -> 128\n",
    "\n",
    "        # Normalize the transformed features\n",
    "        p_transformed = (p_transformed - p_transformed.mean(dim=0)) / p_transformed.std(dim=0)\n",
    "        s_transformed = (s_transformed - s_transformed.mean(dim=0)) / s_transformed.std(dim=0)\n",
    "\n",
    "        # Concatenate the transformed protein and substrate features\n",
    "        combined_features = torch.cat([p_transformed, s_transformed], dim=1)  # Concatenating along feature dimension\n",
    "\n",
    "        # Pass through fully connected layers\n",
    "        out = F.relu(self.fc1(combined_features))\n",
    "        out = self.dropout(out)  # Apply dropout after the first layer\n",
    "\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.fc3(out)  # Output layer\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, optimizer, and loss function\n",
    "protein_input_dim = 2048\n",
    "substrate_input_dim = 1536\n",
    "hidden_dim = 128\n",
    "output_dim = 1  # Binary classification (interaction or no interaction)\n",
    "dropout_prob = 0.5\n",
    "\n",
    "model = NonGraphModel(protein_input_dim, substrate_input_dim, hidden_dim, output_dim, dropout_prob).to(device)\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()  # Binary classification loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if there are common indices between train/val and test sets\n",
    "# common_indices = set(tp_s_test_df.index).intersection(tp_s_train_val_df.index)\n",
    "# if common_indices:\n",
    "#     print(\"Data leakage detected!\")\n",
    "# else:\n",
    "#     print(\"No data leakage found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.4267\n",
      "Epoch [2/20], Loss: 0.1740\n",
      "Epoch [3/20], Loss: 0.1151\n",
      "Epoch [4/20], Loss: 0.0877\n",
      "Epoch [5/20], Loss: 0.0831\n",
      "Epoch [6/20], Loss: 0.0751\n",
      "Epoch [7/20], Loss: 0.0657\n",
      "Epoch [8/20], Loss: 0.0636\n",
      "Epoch [9/20], Loss: 0.0555\n",
      "Epoch [10/20], Loss: 0.0510\n",
      "Epoch [11/20], Loss: 0.0486\n",
      "Epoch [12/20], Loss: 0.0537\n",
      "Epoch [13/20], Loss: 0.0480\n",
      "Epoch [14/20], Loss: 0.0447\n",
      "Epoch [15/20], Loss: 0.0391\n",
      "Epoch [16/20], Loss: 0.0406\n",
      "Epoch [17/20], Loss: 0.0417\n",
      "Epoch [18/20], Loss: 0.0333\n",
      "Epoch [19/20], Loss: 0.0364\n",
      "Epoch [20/20], Loss: 0.0369\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for p_features, s_features, labels, _, _ in train_loader:  # Ignore protein and substrate indices by using '_'\n",
    "        # Move data to the same device as the model\n",
    "        p_features, s_features, labels = p_features.to(device), s_features.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(p_features, s_features)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs.squeeze(), labels)  # Use squeeze to align output and label dimensions\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track the total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0419, Validation AUC: 0.9988\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_loss = 0\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for p_features, s_features, labels, _, _ in loader:  # Ignore protein and substrate indices\n",
    "            p_features, s_features, labels = p_features.to(device), s_features.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(p_features, s_features)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Collect outputs and labels for AUC calculation\n",
    "            all_outputs.append(torch.sigmoid(outputs).cpu().numpy())  # Sigmoid to convert logits to probabilities\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "\n",
    "    # Flatten the list of arrays into a single array\n",
    "    all_outputs = np.concatenate(all_outputs)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "\n",
    "    # Calculate AUC score\n",
    "    auc = roc_auc_score(all_labels, all_outputs)\n",
    "\n",
    "    return avg_loss, auc\n",
    "\n",
    "# Example usage for validation or testing\n",
    "val_loss, val_auc = evaluate(model, val_loader)\n",
    "print(f'Validation Loss: {val_loss:.4f}, Validation AUC: {val_auc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0407, Test AUC: 0.9989\n"
     ]
    }
   ],
   "source": [
    "# After creating the DataLoaders from the split\n",
    "# test_loader is already defined by the previous code\n",
    "\n",
    "# Evaluate on the test set (already split from tp_s_df)\n",
    "test_loss, test_auc = evaluate(model, test_loader)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test AUC: {test_auc:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
