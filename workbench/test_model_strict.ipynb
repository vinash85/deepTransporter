{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch_geometric.data import Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import copy\n",
    "import random\n",
    "import multiprocessing as mp\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable CUDA debugging, use to block cuda calls until completion\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable to use cuda\n",
    "# torch.cuda.set_device(1)  # Use GPU 1\n",
    "\n",
    "# # Verify that the correct GPU is being used\n",
    "# if torch.cuda.is_available():\n",
    "#     current_device = torch.cuda.current_device()\n",
    "#     print(f\"Using GPU: {torch.cuda.get_device_name(current_device)}\")\n",
    "#     print(f\"Device ID: {current_device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load node datasets\n",
    "s_df = pd.read_csv('/data/servilla/DT_HGNN/Model/Nodes/s_emb_full_183.csv', index_col=0) # Substrates\n",
    "p_df = pd.read_csv('/data/servilla/DT_HGNN/Model/Nodes/p_emb_full_237197.csv', index_col=0) # Proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load edge datasets (combined with negative samples)\n",
    "ppi_df = pd.read_csv('/data/servilla/DT_HGNN/Model/Edges/combined_ppi_edges_full.csv') # Protein-Protein Interactions\n",
    "ssi_df = pd.read_csv('/data/servilla/DT_HGNN/Model/Edges/combined_ssi_edges_full.csv') # Substrate-Substrate Interactions\n",
    "tp_s_df = pd.read_csv('/data/servilla/DT_HGNN/Model/Edges/combined_tp_s_edges_full.csv',) # Transporter Protein-Substrate Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect and clean the data, node dataframes should be numeric\n",
    "def inspect_and_clean(df):\n",
    "    non_numeric_columns = df.select_dtypes(exclude=[np.number]).columns\n",
    "    print(f\"Non-numeric columns: {non_numeric_columns}\")\n",
    "    if len(non_numeric_columns) > 0:\n",
    "        df[non_numeric_columns] = df[non_numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "    df = df.fillna(0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numeric columns: Index([], dtype='object')\n",
      "Non-numeric columns: Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "s_df = inspect_and_clean(s_df)\n",
    "p_df = inspect_and_clean(p_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert features to numpy arrays\n",
    "s_features = s_df.values\n",
    "p_features = p_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_features shape: (183, 1536)\n",
      "p_features shape: (237197, 2048)\n"
     ]
    }
   ],
   "source": [
    "# Check shapes to ensure correct dimensions\n",
    "print(f\"s_features shape: {s_features.shape}\")  # Expected (183, 1536)\n",
    "print(f\"p_features shape: {p_features.shape}\")  # Expected (237197, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize node features (normalizes columns to have mean 0 and variance 1)\n",
    "s_features = (s_features - np.mean(s_features, axis=0)) / np.std(s_features, axis=0)\n",
    "p_features = (p_features - np.mean(p_features, axis=0)) / np.std(p_features, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformation layers, changes the number of features 1536 -> 2048\n",
    "# for substrates and 2048 -> 2048 for proteins. The transform_p layer is useful \n",
    "# for transforming the feature representation within the same dimensional space,\n",
    "#  y = Wx + b.\n",
    "\n",
    "device = torch.device('cpu')  # Temporarily switch to CPU\n",
    "\n",
    "\n",
    "transform_s = Linear(1536, 2048).to(device) # Change depending on the number of features\n",
    "transform_p = Linear(2048, 2048).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformations in batches, this can be useful when dealing with large \n",
    "# datasets that may not fit into memory or GPU all at once. \n",
    "def transform_in_batches(features, transform_layer, batch_size=10000):\n",
    "    num_samples = features.shape[0]\n",
    "    print(f\"Number of samples: {num_samples}\")\n",
    "    transformed_features = []\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        batch = features[i:i + batch_size]\n",
    "        batch_tensor = torch.tensor(batch, dtype=torch.float).to(device)\n",
    "        transformed_batch = transform_layer(batch_tensor)\n",
    "        transformed_features.append(transformed_batch.detach().cpu().numpy())  # Use detach() before numpy()\n",
    "    return np.vstack(transformed_features) # Stack arrays in sequence vertically (row wise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 183\n",
      "Number of samples: 237197\n"
     ]
    }
   ],
   "source": [
    "s_features_transformed = transform_in_batches(s_features, transform_s)\n",
    "p_features_transformed = transform_in_batches(p_features, transform_p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to tensors\n",
    "s_features_tensor = torch.tensor(s_features_transformed, dtype=torch.float).to(device)\n",
    "p_features_tensor = torch.tensor(p_features_transformed, dtype=torch.float).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine features, vertically stacks features (dim=0) to create a single tensor\n",
    "all_features = torch.cat([p_features_tensor, s_features_tensor], dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_ids = set(p_df.index)\n",
    "substrate_ids = set(s_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mapping(p_dataframe, s_dataframe):\n",
    "    protein_mapping = {protein_id: i for i, protein_id in enumerate(p_dataframe.index)}\n",
    "    substrate_mapping = {substrate_id: i for i, substrate_id in enumerate(s_dataframe.index)}\n",
    "    return protein_mapping, substrate_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create mappings from the index of the DataFrame since 'Uniprot ID' is the index\n",
    "# protein_mapping = {protein_id: i for i, protein_id in enumerate(p_df.index)}\n",
    "# substrate_mapping = {substrate_id: i for i, substrate_id in enumerate(s_df.index)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_edges_debug(edge_df, source_mapping, target_mapping, edge_type):\n",
    "    edge_df = edge_df.copy()\n",
    "\n",
    "    # Apply mapping\n",
    "    edge_df['source'] = edge_df['source'].map(source_mapping)\n",
    "    edge_df['target'] = edge_df['target'].map(target_mapping)\n",
    "\n",
    "    # Drop rows with NaNs in mapped columns\n",
    "    edge_df.dropna(subset=['source', 'target'], inplace=True)\n",
    "      \n",
    "    return edge_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_mapping, substrate_mapping = get_mapping(p_df, s_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237197\n",
      "First few entries in the protein_mapping dictionary:\n",
      "A0A061ACU2: 0\n",
      "A0A061AE05: 1\n",
      "A0A061I403: 2\n",
      "A0A072ULZ1: 3\n",
      "A0A072VDF2: 4\n",
      "A0A075F7E9: 5\n",
      "A0A075QQ08: 6\n",
      "A0A087WPF7: 7\n",
      "A0A088MLT8: 8\n",
      "A0A089QRB9: 9\n"
     ]
    }
   ],
   "source": [
    "# # Print the entire protein_mapping dictionary\n",
    "# print(\"Protein Mapping Dictionary:\")\n",
    "# for protein, index in protein_mapping.items():\n",
    "#     print(f\"{protein}: {index}\")\n",
    "\n",
    "# Alternatively, to print just the first few entries\n",
    "print(len(protein_mapping))\n",
    "print(\"First few entries in the protein_mapping dictionary:\")\n",
    "for i, (protein, index) in enumerate(protein_mapping.items()):\n",
    "    print(f\"{protein}: {index}\")\n",
    "    if i >= 9:  # Limit to first 10 entries\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183\n",
      "First few entries in the protein_mapping dictionary:\n",
      "CHEBI:30616: 0\n",
      "CHEBI:64837: 1\n",
      "CHEBI:58245: 2\n",
      "CHEBI:57673: 3\n",
      "CHEBI:58115: 4\n",
      "CHEBI:456215: 5\n",
      "CHEBI:58437: 6\n",
      "CHEBI:64716: 7\n",
      "CHEBI:57643: 8\n",
      "CHEBI:72999: 9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Alternatively, to print just the first few entries\n",
    "print(len(substrate_mapping))\n",
    "print(\"First few entries in the protein_mapping dictionary:\")\n",
    "for i, (substrate, index) in enumerate(substrate_mapping.items()):\n",
    "    print(f\"{substrate}: {index}\")\n",
    "    if i >= 9:  # Limit to first 10 entries\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the debugging function to map edges\n",
    "mapped_ppi_edges_df = map_edges_debug(ppi_df, protein_mapping, protein_mapping, 'PPI')\n",
    "mapped_ssi_edges_df = map_edges_debug(ssi_df, substrate_mapping, substrate_mapping, 'SSI')\n",
    "mapped_tp_s_df = map_edges_debug(tp_s_df, protein_mapping, substrate_mapping, 'TP-S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          source  target  label\n",
      "0         157025  225772      1\n",
      "1         187416  234940      1\n",
      "2         234940  187416      1\n",
      "3         225772  100854      1\n",
      "4         225772  157025      1\n",
      "...          ...     ...    ...\n",
      "13327041  106801   24539      0\n",
      "13327042  106199  133771      0\n",
      "13327043   98851   90276      0\n",
      "13327044  191132   11958      0\n",
      "13327045  229284  226859      0\n",
      "\n",
      "[13327046 rows x 3 columns]\n",
      "      source  target  label\n",
      "0         89      17      1\n",
      "1         17      89      1\n",
      "2         99      78      1\n",
      "3         78      99      1\n",
      "4         19     129      1\n",
      "...      ...     ...    ...\n",
      "4349      71      33      0\n",
      "4350      92      97      0\n",
      "4351     140     145      0\n",
      "4352      24     153      0\n",
      "4353      86      81      0\n",
      "\n",
      "[4354 rows x 3 columns]\n",
      "       source  target  label\n",
      "0      110150       0      1\n",
      "1      113110       0      1\n",
      "2       94944      63      1\n",
      "3       86624      63      1\n",
      "4      132714       0      1\n",
      "...       ...     ...    ...\n",
      "26675  121855      84      0\n",
      "26676  168315     165      0\n",
      "26677  160022      19      0\n",
      "26678  209477     122      0\n",
      "26679  230141     115      0\n",
      "\n",
      "[26680 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(mapped_ppi_edges_df)\n",
    "print(mapped_ssi_edges_df)\n",
    "print(mapped_tp_s_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 26630\n",
      "Test set size: 50\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Randomly select 50 samples for the test set from the mapped tp_s_df\n",
    "test_tp_s_df = mapped_tp_s_df.sample(n=50, random_state=42)\n",
    "\n",
    "# Step 2: Remove the selected test samples from the original mapped_tp_s_df to form the training set\n",
    "train_tp_s_df = mapped_tp_s_df.drop(test_tp_s_df.index)\n",
    "\n",
    "# Check the sizes of the resulting DataFrames\n",
    "print(f\"Training set size: {train_tp_s_df.shape[0]}\")\n",
    "print(f\"Test set size: {test_tp_s_df.shape[0]}\")\n",
    "\n",
    "# Step 3: Now, you have train_tp_s_df and test_tp_s_df which can be used for training and testing respectively\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protein training set size: 237147\n",
      "Protein test set size: 50\n",
      "Substrate training set size: 147\n",
      "Substrate test set size: 36\n"
     ]
    }
   ],
   "source": [
    "# Create reverse mappings to go from the index (mapped) back to the original IDs\n",
    "reverse_protein_mapping = {i: protein_id for protein_id, i in protein_mapping.items()}\n",
    "reverse_substrate_mapping = {i: substrate_id for substrate_id, i in substrate_mapping.items()}\n",
    "\n",
    "# Convert the mapped indices back to the original Uniprot IDs and ChEBI IDs\n",
    "test_proteins = test_tp_s_df['source'].map(reverse_protein_mapping).unique()\n",
    "test_substrates = test_tp_s_df['target'].map(reverse_substrate_mapping).unique()\n",
    "\n",
    "# Step 2: Split p_df into train and test based on test_proteins\n",
    "p_test_df = p_df[p_df.index.isin(test_proteins)]  # Test set for proteins\n",
    "p_train_df = p_df[~p_df.index.isin(test_proteins)]  # Train set for proteins\n",
    "\n",
    "# Step 3: Split s_df into train and test based on test_substrates\n",
    "s_test_df = s_df[s_df.index.isin(test_substrates)]  # Test set for substrates\n",
    "s_train_df = s_df[~s_df.index.isin(test_substrates)]  # Train set for substrates\n",
    "\n",
    "# Check the sizes of the resulting DataFrames\n",
    "print(f\"Protein training set size: {p_train_df.shape[0]}\")\n",
    "print(f\"Protein test set size: {p_test_df.shape[0]}\")\n",
    "print(f\"Substrate training set size: {s_train_df.shape[0]}\")\n",
    "print(f\"Substrate test set size: {s_test_df.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the unique protein indices from the test_tp_s_df\n",
    "# test_sources = test_tp_s_df['source'].unique()\n",
    "\n",
    "# # Get the unique substrate indices from the test_tp_s_df\n",
    "# test_targets = test_tp_s_df['target'].unique()\n",
    "\n",
    "# # Remove rows from ppi_df where either source or target is in test_protein_indices\n",
    "# filtered_ppi_df = mapped_ppi_edges_df[~mapped_ppi_edges_df['source'].isin(test_sources) & \n",
    "#                          ~mapped_ppi_edges_df['target'].isin(test_sources)]\n",
    "\n",
    "# # Remove rows from ssi_df where either source or target is in test_substrate_indices\n",
    "# filtered_ssi_df = mapped_ssi_edges_df[~mapped_ssi_edges_df['source'].isin(test_targets) & \n",
    "#                          ~mapped_ssi_edges_df['target'].isin(test_targets)]\n",
    "\n",
    "# # Filter the train_tp_s_df to remove rows where source or target is in the test set\n",
    "# filtered_train_tp_s_df = train_tp_s_df[\n",
    "#     ~train_tp_s_df['source'].isin(test_sources) & \n",
    "#     ~train_tp_s_df['target'].isin(test_targets)\n",
    "# ]\n",
    "\n",
    "# # Check the sizes of the resulting DataFrames after filtering\n",
    "# print(f\"Final size of train_tp_s_df after filtering: {filtered_train_tp_s_df.shape}\")\n",
    "# print(f\"Final size of PPI after filtering: {filtered_ppi_df.shape}\")\n",
    "# print(f\"Final size of SSI after filtering: {filtered_ssi_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_train_mapping, substrate_train_mapping = get_mapping(p_train_df, s_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237147\n",
      "First few entries in the protein_mapping dictionary:\n",
      "A0A061ACU2: 0\n",
      "A0A061AE05: 1\n",
      "A0A061I403: 2\n",
      "A0A072ULZ1: 3\n",
      "A0A072VDF2: 4\n",
      "A0A075F7E9: 5\n",
      "A0A075QQ08: 6\n",
      "A0A087WPF7: 7\n",
      "A0A088MLT8: 8\n",
      "A0A089QRB9: 9\n"
     ]
    }
   ],
   "source": [
    "# Alternatively, to print just the first few entries\n",
    "print(len(protein_train_mapping))\n",
    "print(\"First few entries in the protein_mapping dictionary:\")\n",
    "for i, (protein, index) in enumerate(protein_train_mapping.items()):\n",
    "    print(f\"{protein}: {index}\")\n",
    "    if i >= 9:  # Limit to first 10 entries\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147\n",
      "First few entries in the protein_mapping dictionary:\n",
      "CHEBI:64837: 0\n",
      "CHEBI:58245: 1\n",
      "CHEBI:58115: 2\n",
      "CHEBI:456215: 3\n",
      "CHEBI:58437: 4\n",
      "CHEBI:57643: 5\n",
      "CHEBI:72999: 6\n",
      "CHEBI:83228: 7\n",
      "CHEBI:58950: 8\n",
      "CHEBI:144581: 9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Alternatively, to print just the first few entries\n",
    "print(len(substrate_train_mapping))\n",
    "print(\"First few entries in the protein_mapping dictionary:\")\n",
    "for i, (substrate, index) in enumerate(substrate_train_mapping.items()):\n",
    "    print(f\"{substrate}: {index}\")\n",
    "    if i >= 9:  # Limit to first 10 entries\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the debugging function to map edges\n",
    "mapped_ppi_edges_df = map_edges_debug(ppi_df, protein_train_mapping, protein_train_mapping, 'PPI')\n",
    "mapped_ssi_edges_df = map_edges_debug(ssi_df, substrate_train_mapping, substrate_train_mapping, 'SSI')\n",
    "mapped_tp_s_df = map_edges_debug(tp_s_df, protein_train_mapping, substrate_train_mapping, 'TP-S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            source    target  label\n",
      "0         156986.0  225723.0      1\n",
      "1         187373.0  234890.0      1\n",
      "2         234890.0  187373.0      1\n",
      "3         225723.0  100833.0      1\n",
      "4         225723.0  156986.0      1\n",
      "...            ...       ...    ...\n",
      "13327041  106776.0   24533.0      0\n",
      "13327042  106176.0  133739.0      0\n",
      "13327043   98830.0   90256.0      0\n",
      "13327044  191089.0   11956.0      0\n",
      "13327045  229235.0  226810.0      0\n",
      "\n",
      "[13322673 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(mapped_ppi_edges_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            source    target  label\n",
      "0         156986.0  225723.0      1\n",
      "1         187373.0  234890.0      1\n",
      "2         234890.0  187373.0      1\n",
      "3         225723.0  100833.0      1\n",
      "4         225723.0  156986.0      1\n",
      "...            ...       ...    ...\n",
      "13327041  106776.0   24533.0      0\n",
      "13327042  106176.0  133739.0      0\n",
      "13327043   98830.0   90256.0      0\n",
      "13327044  191089.0   11956.0      0\n",
      "13327045  229235.0  226810.0      0\n",
      "\n",
      "[13322673 rows x 3 columns]\n",
      "      source  target  label\n",
      "0       72.0    13.0      1\n",
      "1       13.0    72.0      1\n",
      "2       79.0    61.0      1\n",
      "3       61.0    79.0      1\n",
      "14     121.0   118.0      1\n",
      "...      ...     ...    ...\n",
      "4348    45.0    29.0      0\n",
      "4349    54.0    25.0      0\n",
      "4351   111.0   115.0      0\n",
      "4352    17.0   122.0      0\n",
      "4353    69.0    64.0      0\n",
      "\n",
      "[2434 rows x 3 columns]\n",
      "         source  target  label\n",
      "13     124256.0    50.0      1\n",
      "26      85773.0    89.0      1\n",
      "57      93838.0    50.0      1\n",
      "76      93422.0    50.0      1\n",
      "79     135469.0     0.0      1\n",
      "...         ...     ...    ...\n",
      "26673   73793.0    59.0      0\n",
      "26674  220487.0     5.0      0\n",
      "26675  121825.0    67.0      0\n",
      "26678  209431.0    97.0      0\n",
      "26679  230092.0    92.0      0\n",
      "\n",
      "[12288 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(mapped_ppi_edges_df)\n",
    "print(mapped_ssi_edges_df)\n",
    "print(mapped_tp_s_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_val_data(df, train_size=0.9, val_size=0.1):\n",
    "    # Split into train and validation (90% train, 10% validation)\n",
    "    train_df, val_df = train_test_split(df, train_size=train_size, random_state=42)\n",
    "    \n",
    "    return train_df, val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for each edge type (without test, as test is predefined)\n",
    "ppi_train_df, ppi_val_df = split_train_val_data(mapped_ppi_edges_df)\n",
    "ssi_train_df, ssi_val_df = split_train_val_data(mapped_ssi_edges_df)\n",
    "tp_s_train_df, tp_s_val_df = split_train_val_data(mapped_tp_s_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply the debugging function to map edges\n",
    "# mapped_ppi_edges_df = map_edges_debug(ppi_df, protein_mapping, protein_mapping, 'PPI')\n",
    "# mapped_ssi_edges_df = map_edges_debug(ssi_df, substrate_mapping, substrate_mapping, 'SSI')\n",
    "# mapped_tp_s_df = map_edges_debug(train_tp_s_df, protein_mapping, substrate_mapping, 'TP-S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create edge index tensors for training, validation, and test\n",
    "train_edges_tp_s = torch.tensor(tp_s_train_df[['source', 'target']].values.T, dtype=torch.long)\n",
    "val_edges_tp_s = torch.tensor(tp_s_val_df[['source', 'target']].values.T, dtype=torch.long)\n",
    "test_edges_tp_s = torch.tensor(test_tp_s_df[['source', 'target']].values.T, dtype=torch.long)\n",
    "\n",
    "train_edges_ppi = torch.tensor(ppi_train_df[['source', 'target']].values.T, dtype=torch.long)\n",
    "val_edges_ppi = torch.tensor(ppi_val_df[['source', 'target']].values.T, dtype=torch.long)\n",
    "\n",
    "train_edges_ssi = torch.tensor(ssi_train_df[['source', 'target']].values.T, dtype=torch.long)\n",
    "val_edges_ssi = torch.tensor(ssi_val_df[['source', 'target']].values.T, dtype=torch.long)\n",
    "\n",
    "# Labels remain the same\n",
    "train_labels_tp_s = torch.tensor(tp_s_train_df['label'].values, dtype=torch.float)\n",
    "val_labels_tp_s = torch.tensor(tp_s_val_df['label'].values, dtype=torch.float)\n",
    "test_labels_tp_s = torch.tensor(test_tp_s_df['label'].values, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(147, 1536)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (147x1536 and 2048x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[279], line 106\u001b[0m\n\u001b[1;32m    103\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m800\u001b[39m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# Training step\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     train_loss, train_preds \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_edges_tp_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels_tp_s\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# Validation step\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     val_loss, val_preds \u001b[38;5;241m=\u001b[39m validate()\n",
      "Cell \u001b[0;32mIn[279], line 63\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(x_dict, edge_index_dict, train_edges_tp_s, train_labels_tp_s)\u001b[0m\n\u001b[1;32m     61\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     62\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 63\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_edges_tp_s\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, train_labels_tp_s)\n\u001b[1;32m     65\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/MLG/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/MLG/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[279], line 34\u001b[0m, in \u001b[0;36mGCNLinkPredictor.forward\u001b[0;34m(self, x_dict, edge_index_dict, edges)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_dict, edge_index_dict, edges):\n\u001b[0;32m---> 34\u001b[0m     z_protein, z_substrate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     z_combined \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([z_protein[edges[\u001b[38;5;241m0\u001b[39m]], z_substrate[edges[\u001b[38;5;241m1\u001b[39m]]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlink_predictor(z_combined)\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "Cell \u001b[0;32mIn[279], line 30\u001b[0m, in \u001b[0;36mGCNLinkPredictor.encode\u001b[0;34m(self, x_dict, edge_index_dict)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_dict, edge_index_dict):\n\u001b[1;32m     29\u001b[0m     z_protein \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprotein_conv1(x_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprotein\u001b[39m\u001b[38;5;124m'\u001b[39m], edge_index_dict[(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprotein\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minteracts_with\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprotein\u001b[39m\u001b[38;5;124m'\u001b[39m)])\n\u001b[0;32m---> 30\u001b[0m     z_substrate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubstrate_conv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msubstrate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msubstrate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minteracts_with\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msubstrate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m z_protein, z_substrate\n",
      "File \u001b[0;32m~/miniconda3/envs/MLG/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/MLG/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/MLG/lib/python3.8/site-packages/torch_geometric/nn/conv/gcn_conv.py:260\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m             edge_index \u001b[38;5;241m=\u001b[39m cache\n\u001b[0;32m--> 260\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: Tensor, edge_weight: OptTensor)\u001b[39;00m\n\u001b[1;32m    263\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpropagate(edge_index, x\u001b[38;5;241m=\u001b[39mx, edge_weight\u001b[38;5;241m=\u001b[39medge_weight)\n",
      "File \u001b[0;32m~/miniconda3/envs/MLG/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/MLG/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/MLG/lib/python3.8/site-packages/torch_geometric/nn/dense/linear.py:147\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    142\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m        x (torch.Tensor): The input features.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (147x1536 and 2048x64)"
     ]
    }
   ],
   "source": [
    "data = HeteroData()\n",
    "\n",
    "# Assign node features\n",
    "# During training, only use the node features corresponding to the training set\n",
    "data['protein'].x = torch.tensor(p_train_df.values, dtype=torch.float).to(device)\n",
    "data['substrate'].x = torch.tensor(s_train_df.values, dtype=torch.float).to(device)\n",
    "\n",
    "# Assign training edges\n",
    "data['protein', 'interacts_with', 'substrate'].edge_index = train_edges_tp_s\n",
    "data['protein', 'interacts_with', 'protein'].edge_index = train_edges_ppi\n",
    "data['substrate', 'interacts_with', 'substrate'].edge_index = train_edges_ssi\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "class GCNLinkPredictor(nn.Module):\n",
    "    def __init__(self, protein_dim, substrate_dim, hidden_channels):\n",
    "        super(GCNLinkPredictor, self).__init__()\n",
    "        self.protein_conv1 = GCNConv(protein_dim, hidden_channels)\n",
    "        self.substrate_conv1 = GCNConv(substrate_dim, hidden_channels)\n",
    "        self.protein_conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.substrate_conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.link_predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_channels * 2, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels, 1)\n",
    "        )\n",
    "\n",
    "    def encode(self, x_dict, edge_index_dict):\n",
    "        z_protein = self.protein_conv1(x_dict['protein'], edge_index_dict[('protein', 'interacts_with', 'protein')])\n",
    "        z_substrate = self.substrate_conv1(x_dict['substrate'], edge_index_dict[('substrate', 'interacts_with', 'substrate')])\n",
    "        return z_protein, z_substrate\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, edges):\n",
    "        z_protein, z_substrate = self.encode(x_dict, edge_index_dict)\n",
    "        z_combined = torch.cat([z_protein[edges[0]], z_substrate[edges[1]]], dim=-1)\n",
    "        return self.link_predictor(z_combined).squeeze()\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = GCNLinkPredictor(protein_dim=2048, substrate_dim=2048, hidden_channels=64).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 10  # Number of epochs to wait before stopping if no improvement\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=4)\n",
    "\n",
    "\n",
    "# Assuming `data` contains x_dict and edge_index_dict\n",
    "x_dict = data.x_dict\n",
    "edge_index_dict = data.edge_index_dict\n",
    "\n",
    "\n",
    "\n",
    "# Update the train, validate, and test functions as needed\n",
    "def train(x_dict, edge_index_dict, train_edges_tp_s, train_labels_tp_s): \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(x_dict, edge_index_dict, train_edges_tp_s)\n",
    "    loss = criterion(out, train_labels_tp_s)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item(), out.detach()\n",
    "\n",
    "def validate():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x_dict, data.edge_index_dict, val_edges_tp_s)\n",
    "        loss = criterion(out, val_labels_tp_s)\n",
    "    return loss.item(), out\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x_dict, data.edge_index_dict, test_edges_tp_s)\n",
    "        loss = criterion(out, test_labels_tp_s)\n",
    "    return loss.item(), out\n",
    "\n",
    "# Calculate additional metrics\n",
    "def calculate_metrics(labels, preds):\n",
    "    preds = torch.sigmoid(preds).cpu().numpy()\n",
    "    preds_binary = (preds > 0.5).astype(int)\n",
    "    labels = labels.cpu().numpy()\n",
    "\n",
    "    accuracy = accuracy_score(labels, preds_binary)\n",
    "    precision = precision_score(labels, preds_binary)\n",
    "    recall = recall_score(labels, preds_binary)\n",
    "    f1 = f1_score(labels, preds_binary)\n",
    "    auc = roc_auc_score(labels, preds)\n",
    "\n",
    "    return accuracy, precision, recall, f1, auc\n",
    "\n",
    "# Modify the training loop to include metric calculation and visualization\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Training loop\n",
    "epochs = 800\n",
    "for epoch in range(epochs):\n",
    "    # Training step\n",
    "    train_loss, train_preds = train(x_dict, edge_index_dict, train_edges_tp_s, train_labels_tp_s)\n",
    "    # Validation step\n",
    "    val_loss, val_preds = validate()\n",
    "\n",
    "    # Store losses\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # Calculate validation metrics\n",
    "    accuracy, precision, recall, f1, auc = calculate_metrics(val_labels_tp_s, val_preds)\n",
    "    val_accuracies.append(accuracy)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, \"\n",
    "          f\"Val Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, \"\n",
    "          f\"F1: {f1:.4f}, AUC: {auc:.4f}, LR: {scheduler.get_last_lr()[0]}\")\n",
    "\n",
    "    # Step the LR scheduler\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Check for early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_without_improvement = 0\n",
    "        torch.save(model.state_dict(), '/data/servilla/DT_HGNN/data/Models_saves/best_model.pth')  # Save the best model\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "    \n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('/data/servilla/DT_HGNN/data/Models_saves/best_model.pth'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "# During testing, use the full node feature set for both proteins and substrates\n",
    "data['protein'].x = torch.tensor(p_df.values, dtype=torch.float).to(device)\n",
    "data['substrate'].x = torch.tensor(s_df.values, dtype=torch.float).to(device)\n",
    "\n",
    "# Use testing edges\n",
    "data['protein', 'interacts_with', 'substrate'].edge_index = test_edges_tp_s\n",
    "# data['protein', 'interacts_with', 'protein'].edge_index = test_edges_ppi  # if needed for testing\n",
    "# data['substrate', 'interacts_with', 'substrate'].edge_index = test_edges_ssi  # if needed for testing\n",
    "\n",
    "\n",
    "test_loss, test_preds = test()\n",
    "test_accuracy, test_precision, test_recall, test_f1, test_auc = calculate_metrics(test_labels_tp_s, test_preds)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}, AUC: {test_auc:.4f}\")\n",
    "\n",
    "# Step 4: Plot loss curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss Curves\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Step 5: Plot validation accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(val_accuracies, label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Protein indices out of bounds in train_edges_tp_s",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[143], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m max_protein_idx \u001b[38;5;241m=\u001b[39m p_train_df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Maximum index for proteins\u001b[39;00m\n\u001b[1;32m      9\u001b[0m max_substrate_idx \u001b[38;5;241m=\u001b[39m s_train_df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Maximum index for substrates\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m verify_edge_indices(train_edges_tp_s[\u001b[38;5;241m0\u001b[39m], max_protein_idx), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProtein indices out of bounds in train_edges_tp_s\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m verify_edge_indices(train_edges_tp_s[\u001b[38;5;241m1\u001b[39m], max_substrate_idx), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSubstrate indices out of bounds in train_edges_tp_s\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Repeat for validation and test sets\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Protein indices out of bounds in train_edges_tp_s"
     ]
    }
   ],
   "source": [
    "# Step 1: Verify that node indices are within bounds\n",
    "# Ensure that edges in train_edges_tp_s, test_edges_tp_s are within the size of the node feature tensors\n",
    "\n",
    "def verify_edge_indices(edge_tensor, max_index):\n",
    "    return (edge_tensor >= 0).all() and (edge_tensor < max_index).all()\n",
    "\n",
    "# Check training and validation edges for protein and substrate sets\n",
    "max_protein_idx = p_train_df.shape[0]  # Maximum index for proteins\n",
    "max_substrate_idx = s_train_df.shape[0]  # Maximum index for substrates\n",
    "\n",
    "assert verify_edge_indices(train_edges_tp_s[0], max_protein_idx), \"Protein indices out of bounds in train_edges_tp_s\"\n",
    "assert verify_edge_indices(train_edges_tp_s[1], max_substrate_idx), \"Substrate indices out of bounds in train_edges_tp_s\"\n",
    "\n",
    "# Repeat for validation and test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid protein indices: tensor([237187, 237164])\n"
     ]
    }
   ],
   "source": [
    "# Check for out-of-bound protein indices\n",
    "invalid_protein_indices = train_edges_tp_s[0][train_edges_tp_s[0] >= max_protein_idx]\n",
    "print(f\"Invalid protein indices: {invalid_protein_indices}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
