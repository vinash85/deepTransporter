{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch_geometric.data import Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import copy\n",
    "import random\n",
    "import multiprocessing as mp\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, precision_recall_curve, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.loader import DataLoader  # For loading graphs in batches\n",
    "from torch_geometric.utils import negative_sampling  # For handling link prediction tasks\n",
    "from torch_geometric.utils import to_undirected\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from datetime import datetime\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load node data from csv\n",
    "s_df = pd.read_csv('/data/servilla/DT_HGNN/Model/Nodes/s_emb_full_183.csv', index_col=0)\n",
    "p_df = pd.read_csv('/data/servilla/DT_HGNN/Model/Nodes/p_emb_full_237197.csv', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load edge data from csv\n",
    "tp_s_df = pd.read_csv('/data/servilla/DT_HGNN/Model/Edges/distributed_combined_tp_s_edges_13340.csv')\n",
    "ppi_df = pd.read_csv('/data/servilla/DT_HGNN/Model/Edges/combined_ppi_edges_full.csv')\n",
    "ssi_df = pd.read_csv('/data/servilla/DT_HGNN/Model/Edges/combined_ssi_edges_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load device of available GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == 'cuda':\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(device)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Definitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the current date and time for the file name and log\n",
    "def get_timestamp():\n",
    "    return datetime.now().strftime('%Y-%m-%d_%H-%M-%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect and clean the data, converts non-numeric columns to numeric and fills NaN values with 0\n",
    "def inspect_and_clean(df):\n",
    "    non_numeric_columns = df.select_dtypes(exclude=[np.number]).columns\n",
    "    print(f\"Non-numeric columns: {non_numeric_columns}\")\n",
    "    if len(non_numeric_columns) > 0:\n",
    "        df[non_numeric_columns] = df[non_numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "    df = df.fillna(0)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformations in batches, this can be useful when dealing with large \n",
    "# datasets that may not fit into memory or GPU all at once. \n",
    "def transform_in_batches(features, transform_layer, batch_size=10000):\n",
    "    num_samples = features.shape[0]\n",
    "    print(f\"Number of samples: {num_samples}\")\n",
    "    transformed_features = []\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        batch = features[i:i + batch_size]\n",
    "        batch_tensor = torch.tensor(batch, dtype=torch.float).to(device)\n",
    "        transformed_batch = transform_layer(batch_tensor)\n",
    "        transformed_features.append(transformed_batch.detach().cpu().numpy())  # Use detach() before numpy()\n",
    "    return np.vstack(transformed_features) # Stack arrays in sequence vertically (row wise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, train_size=0.8, val_size=0.1, test_size=0.1, random_state=42):\n",
    "    # Shuffle and split the data into train, validation, and test sets at once\n",
    "    # Calculate the remaining size after the train split (val + test)\n",
    "    train_df, temp_df = train_test_split(df, train_size=train_size, random_state=random_state)\n",
    "    \n",
    "    # Calculate the size for validation and test splits from the remaining data\n",
    "    val_test_ratio = val_size / (val_size + test_size)\n",
    "    \n",
    "    # Split the remaining temp_df into validation and test sets\n",
    "    val_df, test_df = train_test_split(temp_df, train_size=val_test_ratio, random_state=random_state)\n",
    "    \n",
    "    return train_df, val_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_correct_mapping(df, source_mapping, target_mapping):\n",
    "    # Apply the mappings to 'source' and 'target'\n",
    "    df['mapped_source'] = df['source'].map(source_mapping)\n",
    "    df['mapped_target'] = df['target'].map(target_mapping)\n",
    "\n",
    "    # Identify rows where mapping failed (NaN values)\n",
    "    unmapped_sources = df[df['mapped_source'].isna()]['source'].unique()\n",
    "    unmapped_targets = df[df['mapped_target'].isna()]['target'].unique()\n",
    "\n",
    "    # Log or print unmapped elements\n",
    "    if len(unmapped_sources) > 0:\n",
    "        print(f\"Unmapped sources: {unmapped_sources}\")\n",
    "    if len(unmapped_targets) > 0:\n",
    "        print(f\"Unmapped targets: {unmapped_targets}\")\n",
    "\n",
    "    # Remove rows where mapping failed (NaN values)\n",
    "    df.dropna(subset=['mapped_source', 'mapped_target'], inplace=True)\n",
    "\n",
    "    # Replace original 'source' and 'target' with mapped values and drop the extra columns\n",
    "    df['source'] = df['mapped_source']\n",
    "    df['target'] = df['mapped_target']\n",
    "    df.drop(columns=['mapped_source', 'mapped_target'], inplace=True)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert edges to undirected and combine symmetric labels using 'reduce'\n",
    "def undirected(edges_df, label_column, reduce='mean'):\n",
    "    # Ensure 'source' and 'target' are numeric\n",
    "    edges_df['source'] = pd.to_numeric(edges_df['source'], errors='coerce')\n",
    "    edges_df['target'] = pd.to_numeric(edges_df['target'], errors='coerce')\n",
    "\n",
    "    # Check for NaN values (if any elements could not be converted)\n",
    "    if edges_df[['source', 'target']].isna().any().any():\n",
    "        raise ValueError(\"Some source or target nodes could not be converted to numeric values.\")\n",
    "\n",
    "    # Convert to torch tensor\n",
    "    edge_index = torch.tensor(edges_df[['source', 'target']].values.T, dtype=torch.long)\n",
    "\n",
    "    # Extract the labels (weights) as a tensor\n",
    "    edge_attr = torch.tensor(edges_df[label_column].values, dtype=torch.float)\n",
    "\n",
    "    # Convert to undirected edges and combine labels\n",
    "    edge_index, edge_attr = to_undirected(edge_index, edge_attr=edge_attr, reduce=reduce)\n",
    "\n",
    "    # Create a new DataFrame with expanded edges and combined labels\n",
    "    new_edges_df = pd.DataFrame({\n",
    "        'source': edge_index[0].numpy(),\n",
    "        'target': edge_index[1].numpy(),\n",
    "        label_column: edge_attr.numpy()  # Store the combined labels\n",
    "    })\n",
    "    \n",
    "    return new_edges_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing, we can remove percentage of edges from the each of the training sets\n",
    "def remove_edges(edges, removal_percentage):\n",
    "    num_edges = edges.size(1)\n",
    "    num_to_remove = int(removal_percentage * num_edges)\n",
    "    indices_to_keep = torch.randperm(num_edges)[:-num_to_remove]  # Randomly select edges to keep\n",
    "    return edges[:, indices_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to manipulate data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numeric columns: Index([], dtype='object')\n",
      "Non-numeric columns: Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "s_df = inspect_and_clean(s_df)\n",
    "p_df = inspect_and_clean(p_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert features to numpy arrays\n",
    "s_features = s_df.values\n",
    "p_features = p_df.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of s_features: (183, 1536)\n",
      "Shape of p_features: (12301, 2048)\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of the features\n",
    "print(f\"Shape of s_features: {s_features.shape}\")\n",
    "print(f\"Shape of p_features: {p_features.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate mappings\n",
    "protein_mapping = {node_id: i for i, node_id in enumerate(p_df.index)}\n",
    "substrate_mapping = {node_id: i for i, node_id in enumerate(s_df.index)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmapped sources: ['Q9ZFK1' 'P70910' 'P70911' ... 'A4G0Y3' 'A4FZL6' 'A4FZY5']\n",
      "Unmapped targets: ['Q8VS69' 'P70911' 'P70910' ... 'P36370' 'A4FWY9' 'A4FW54']\n"
     ]
    }
   ],
   "source": [
    "# Apply mappings to the full edge DataFrames before splitting\n",
    "tp_s_df = apply_correct_mapping(tp_s_df, protein_mapping, substrate_mapping)\n",
    "ppi_df = apply_correct_mapping(ppi_df, protein_mapping, protein_mapping)\n",
    "ssi_df = apply_correct_mapping(ssi_df, substrate_mapping, substrate_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_s_df = undirected(tp_s_df, label_column='label', reduce='mean')\n",
    "ppi_df = undirected(ppi_df, label_column='label', reduce='mean')\n",
    "ssi_df = undirected(ssi_df, label_column='label', reduce='mean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for each edge type\n",
    "ppi_train_df, ppi_val_df, ppi_test_df = split_data(ppi_df)\n",
    "ssi_train_df, ssi_val_df, ssi_test_df = split_data(ssi_df)\n",
    "tp_s_train_df, tp_s_val_df, tp_s_test_df = split_data(tp_s_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create edge index tensors\n",
    "train_edges_tp_s = torch.tensor(tp_s_train_df[['source', 'target']].values.T, dtype=torch.long)\n",
    "val_edges_tp_s = torch.tensor(tp_s_val_df[['source', 'target']].values.T, dtype=torch.long)\n",
    "test_edges_tp_s = torch.tensor(tp_s_test_df[['source', 'target']].values.T, dtype=torch.long)\n",
    "\n",
    "train_edges_ppi = torch.tensor(ppi_train_df[['source', 'target']].values.T, dtype=torch.long)\n",
    "val_edges_ppi = torch.tensor(ppi_val_df[['source', 'target']].values.T, dtype=torch.long)\n",
    "test_edges_ppi = torch.tensor(ppi_test_df[['source', 'target']].values.T, dtype=torch.long)\n",
    "\n",
    "train_edges_ssi = torch.tensor(ssi_train_df[['source', 'target']].values.T, dtype=torch.long)\n",
    "val_edges_ssi = torch.tensor(ssi_val_df[['source', 'target']].values.T, dtype=torch.long)\n",
    "test_edges_ssi = torch.tensor(ssi_test_df[['source', 'target']].values.T, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the labels to tensors\n",
    "train_labels_tp_s = torch.tensor(tp_s_train_df['label'].values, dtype=torch.float)\n",
    "val_labels_tp_s = torch.tensor(tp_s_val_df['label'].values, dtype=torch.float)\n",
    "test_labels_tp_s = torch.tensor(tp_s_test_df['label'].values, dtype=torch.float)\n",
    "\n",
    "train_labels_ppi = torch.tensor(ppi_train_df['label'].values, dtype=torch.float)\n",
    "val_labels_ppi = torch.tensor(ppi_val_df['label'].values, dtype=torch.float)\n",
    "test_labels_ppi = torch.tensor(ppi_test_df['label'].values, dtype=torch.float)\n",
    "\n",
    "train_labels_ssi = torch.tensor(ssi_train_df['label'].values, dtype=torch.float)\n",
    "val_labels_ssi = torch.tensor(ssi_val_df['label'].values, dtype=torch.float)\n",
    "test_labels_ssi = torch.tensor(ssi_test_df['label'].values, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data into positive samples\n",
    "positive_tp_s_train_df = tp_s_train_df[tp_s_train_df['label'] == 1]\n",
    "\n",
    "train_edges_tp_s_positive = torch.tensor(positive_tp_s_train_df[['source', 'target']].values.T, dtype=torch.long)\n",
    "\n",
    "positive_ppi_train_df = ppi_train_df[ppi_train_df['label'] == 1]\n",
    "\n",
    "train_edges_ppi_positive = torch.tensor(positive_ppi_train_df[['source', 'target']].values.T, dtype=torch.long)\n",
    "\n",
    "positive_ssi_train_df = ssi_train_df[ssi_train_df['label'] == 1]\n",
    "\n",
    "train_edges_ssi_positive = torch.tensor(positive_ssi_train_df[['source', 'target']].values.T, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing, we can remove percentage of edges from the training set\n",
    "# Define the percentage of edges to remove\n",
    "ppi_removal_percentage = 1.0  # Remove 20% of PPI edges\n",
    "ssi_removal_percentage = 1.0  # Remove 20% of SSI edges\n",
    "tp_s_removal_percentage = 1.0  # Remove 20% of tp_s edges\n",
    "\n",
    "# Remove a percentage of PPI edges\n",
    "train_edges_ppi_positive_reduced = remove_edges(train_edges_ppi_positive, ppi_removal_percentage)\n",
    "\n",
    "# Remove a percentage of SSI edges\n",
    "train_edges_ssi_positive_reduced = remove_edges(train_edges_ssi_positive, ssi_removal_percentage)\n",
    "\n",
    "# Remove a percentage of tp_s edges\n",
    "train_edges_tp_s_positive_reduced = remove_edges(train_edges_tp_s_positive, tp_s_removal_percentage)\n",
    "\n",
    "train_edges_tp_s_reduced = remove_edges(train_edges_tp_s, tp_s_removal_percentage)\n",
    "\n",
    "val_edges_tp_s_reduced = remove_edges(val_edges_tp_s, tp_s_removal_percentage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the features to tensors (first convert to numpy arrays)\n",
    "s_np = s_df.values\n",
    "p_np = p_df.values\n",
    "\n",
    "s_features_tensor = torch.tensor(s_np, dtype=torch.float).to(device)\n",
    "p_features_tensor = torch.tensor(p_np, dtype=torch.float).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "data = HeteroData()\n",
    "\n",
    "# Assign node features\n",
    "data['protein'].x = p_features_tensor\n",
    "data['substrate'].x = s_features_tensor\n",
    "\n",
    "# Create HeteroData for training\n",
    "train_data = HeteroData()\n",
    "\n",
    "# Add node features (assumed the same for training and validation)\n",
    "train_data['protein'].x = data['protein'].x\n",
    "train_data['substrate'].x = data['substrate'].x\n",
    "\n",
    "# Add training edges for tp_s, ppi, and ssi\n",
    "train_data['protein', 'interacts_with', 'substrate'].edge_index = train_edges_tp_s_reduced\n",
    "train_data['protein', 'interacts_with', 'protein'].edge_index = train_edges_ppi_positive\n",
    "train_data['substrate', 'interacts_with', 'substrate'].edge_index = train_edges_ssi_positive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HeteroData for validation\n",
    "val_data = HeteroData()\n",
    "\n",
    "# Add node features\n",
    "val_data['protein'].x = data['protein'].x\n",
    "val_data['substrate'].x = data['substrate'].x\n",
    "\n",
    "# Add validation edges\n",
    "val_data['protein', 'interacts_with', 'substrate'].edge_index = val_edges_tp_s\n",
    "val_data['protein', 'interacts_with', 'protein'].edge_index = val_edges_ppi\n",
    "val_data['substrate', 'interacts_with', 'substrate'].edge_index = val_edges_ssi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HeteroData for testing\n",
    "test_data = HeteroData()\n",
    "\n",
    "# Add node features (assumed same as training/validation)\n",
    "test_data['protein'].x = data['protein'].x\n",
    "test_data['substrate'].x = data['substrate'].x\n",
    "\n",
    "# Add test edges\n",
    "test_data['protein', 'interacts_with', 'substrate'].edge_index = test_edges_tp_s\n",
    "test_data['protein', 'interacts_with', 'protein'].edge_index = test_edges_ppi\n",
    "test_data['substrate', 'interacts_with', 'substrate'].edge_index = test_edges_ssi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGEModel(torch.nn.Module):\n",
    "    def __init__(self, protein_input_dim, substrate_input_dim, hidden_dim, output_dim, dropout_prob=0.5):\n",
    "        super(GraphSAGEModel, self).__init__()\n",
    "        \n",
    "        # Transformation layers to match input dimensions\n",
    "        self.transform_p = Linear(protein_input_dim, hidden_dim)  # Protein: 2048 -> 128\n",
    "        self.transform_s = Linear(substrate_input_dim, hidden_dim)  # Substrate: 1536 -> 128\n",
    "        \n",
    "        # GraphSAGE convolution layers\n",
    "        self.sage_conv1 = SAGEConv(hidden_dim, hidden_dim)\n",
    "        self.sage_conv2 = SAGEConv(hidden_dim, output_dim)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = torch.nn.Dropout(p=dropout_prob)  # Dropout probability, usually set to 0.5\n",
    "        \n",
    "    def forward(self, data):\n",
    "        # Get the protein and substrate features from HeteroData\n",
    "        p_features = data['protein'].x\n",
    "        s_features = data['substrate'].x\n",
    "        \n",
    "        # Apply transformations to ensure both node types have the same dimensionality\n",
    "        p_transformed = self.transform_p(p_features)  # Protein features: 2048 -> 128\n",
    "        s_transformed = self.transform_s(s_features)  # Substrate features: 1536 -> 128\n",
    "\n",
    "        # Normalize the transformed features\n",
    "        p_transformed = (p_transformed - p_transformed.mean(dim=0)) / p_transformed.std(dim=0)\n",
    "        s_transformed = (s_transformed - s_transformed.mean(dim=0)) / s_transformed.std(dim=0)\n",
    "        \n",
    "        # Combine the transformed features\n",
    "        all_features = torch.cat([p_transformed, s_transformed], dim=0)\n",
    "\n",
    "        # # Print edge shapes to verify their dimensions\n",
    "        # print(f\"Protein-Substrate edge shape: {data['protein', 'interacts_with', 'substrate'].edge_index.shape}\")\n",
    "\n",
    "        \n",
    "        # Apply GraphSAGE convolution layers\n",
    "        out = F.relu(self.sage_conv1(all_features, data['protein', 'interacts_with', 'substrate'].edge_index))\n",
    "        out = self.dropout(out)  # Apply dropout after the first convolution layer\n",
    "        \n",
    "        # Second GraphSAGE convolution\n",
    "        out = self.sage_conv2(out, data['protein', 'interacts_with', 'substrate'].edge_index)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the device (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the model and move it to the device (GPU or CPU)\n",
    "model = GraphSAGEModel(protein_input_dim=2048, substrate_input_dim=1536, hidden_dim=128, output_dim=64).to(device)\n",
    "\n",
    "# Move your HeteroData to the same device\n",
    "data = data.to(device)\n",
    "\n",
    "# Move labels and data to device\n",
    "train_labels_tp_s = train_labels_tp_s.to(device)\n",
    "val_labels_tp_s = val_labels_tp_s.to(device)\n",
    "train_data = train_data.to(device)\n",
    "val_data = val_data.to(device)\n",
    "\n",
    "# Initialize optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "early_stopping_patience = 10\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train_edges_tp_s_reduced size: torch.Size([2, 0])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "continuous format is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m train_probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(edge_scores)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     50\u001b[0m train_labels_np \u001b[38;5;241m=\u001b[39m train_labels_tp_s\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m---> 51\u001b[0m train_auc \u001b[38;5;241m=\u001b[39m \u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_labels_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_probs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m train_aucs\u001b[38;5;241m.\u001b[39mappend(train_auc)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Training Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Training AUC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_auc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, LR: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscheduler\u001b[38;5;241m.\u001b[39mget_last_lr()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/MLG/lib/python3.8/site-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n\u001b[0;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    224\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/MLG/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:635\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _average_binary_score(\n\u001b[1;32m    628\u001b[0m         partial(_binary_roc_auc_score, max_fpr\u001b[38;5;241m=\u001b[39mmax_fpr),\n\u001b[1;32m    629\u001b[0m         y_true,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    632\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[1;32m    633\u001b[0m     )\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# multilabel-indicator\u001b[39;00m\n\u001b[0;32m--> 635\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_average_binary_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_binary_roc_auc_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_fpr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_fpr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/MLG/lib/python3.8/site-packages/sklearn/metrics/_base.py:72\u001b[0m, in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     70\u001b[0m y_type \u001b[38;5;241m=\u001b[39m type_of_target(y_true)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-indicator\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m format is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y_type))\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m binary_metric(y_true, y_score, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "\u001b[0;31mValueError\u001b[0m: continuous format is not supported"
     ]
    }
   ],
   "source": [
    "# Set up early stopping and checkpointing\n",
    "early_stopping_patience = 10\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "# Initialize lists to store training metrics\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_aucs = []\n",
    "val_aucs = []\n",
    "\n",
    "# Define your hyperparameters in a dictionary\n",
    "hyperparameters = {\n",
    "    'learning_rate': 1e-3,\n",
    "    'weight_decay': 1e-5,\n",
    "    'hidden_dim': 128,\n",
    "    'output_dim': 64,\n",
    "    'dropout_prob': 0.5,\n",
    "    'optimizer': 'Adam'\n",
    "}\n",
    "\n",
    "# Initialize optimizer and learning rate scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hyperparameters['learning_rate'], weight_decay=hyperparameters['weight_decay'])\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=100)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Print the size of train_edges_tp_s_reduced\n",
    "    print(f'Epoch {epoch}, train_edges_tp_s_reduced size: {train_edges_tp_s_reduced.size()}')\n",
    "    \n",
    "    # Forward pass\n",
    "    train_out = model(train_data)\n",
    "    source_embeddings = train_out[train_edges_tp_s[0]]\n",
    "    target_embeddings = train_out[train_edges_tp_s[1]]\n",
    "    edge_scores = (source_embeddings * target_embeddings).sum(dim=1)\n",
    "    train_loss = criterion(edge_scores, train_labels_tp_s)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Save training loss\n",
    "    train_losses.append(train_loss.item())\n",
    "    \n",
    "    # Compute AUC for training data\n",
    "    train_probs = torch.sigmoid(edge_scores).detach().cpu().numpy()\n",
    "    train_labels_np = train_labels_tp_s.cpu().numpy()\n",
    "    train_auc = roc_auc_score(train_labels_np, train_probs)\n",
    "    train_aucs.append(train_auc)\n",
    "\n",
    "    print(f'Epoch {epoch}, Training Loss: {train_loss.item()}, Training AUC: {train_auc}, LR: {scheduler.get_last_lr()[0]}')\n",
    "\n",
    "    # Validation pass\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_out = model(val_data)\n",
    "        val_source_embeddings = val_out[val_edges_tp_s[0]]\n",
    "        val_target_embeddings = val_out[val_edges_tp_s[1]]\n",
    "        val_edge_scores = (val_source_embeddings * val_target_embeddings).sum(dim=1)\n",
    "        val_loss = criterion(val_edge_scores, val_labels_tp_s)\n",
    "\n",
    "        # Save validation loss\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "        # Compute AUC for validation data\n",
    "        val_probs = torch.sigmoid(val_edge_scores).cpu().numpy()\n",
    "        val_labels_np = val_labels_tp_s.cpu().numpy()\n",
    "        val_auc = roc_auc_score(val_labels_np, val_probs)\n",
    "        val_aucs.append(val_auc)\n",
    "\n",
    "        print(f'Validation Loss: {val_loss.item()}, Validation AUC: {val_auc}')\n",
    "        \n",
    "        # Early Stopping Logic\n",
    "        if val_loss.item() < best_val_loss:\n",
    "            best_val_loss = val_loss.item()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f'Epoch {epoch}, Validation Loss: {val_loss.item():.4f}. Patience Counter: {patience_counter}')\n",
    "\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(f'Early stopping triggered at epoch {epoch}')\n",
    "            break\n",
    "\n",
    "    # Step the scheduler with the validation loss\n",
    "    scheduler.step(val_loss.item())\n",
    "\n",
    "    model.train()  # Switch back to training mode\n",
    "\n",
    "# Add timestamp to checkpoint path at the end of training\n",
    "final_checkpoint_path = f'/data/servilla/DT_HGNN/Model/Model_Checkpoints/GraphSAGE/{get_timestamp()}_best_model.pt'\n",
    "\n",
    "# Save the final model checkpoint with all the data\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scheduler_state_dict': scheduler.state_dict(),  # Save scheduler state\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'train_aucs': train_aucs,\n",
    "    'val_aucs': val_aucs,\n",
    "    'best_val_loss': best_val_loss,\n",
    "    'epoch': epoch,\n",
    "    'timestamp': get_timestamp(),\n",
    "    'hyperparameters': hyperparameters\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, final_checkpoint_path)\n",
    "print(f\"Final model and training data saved as {final_checkpoint_path}\")\n",
    "\n",
    "# Plot Training and Validation Loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Time')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Training and Validation AUC\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_aucs, label='Training AUC')\n",
    "plt.plot(val_aucs, label='Validation AUC')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('AUC')\n",
    "plt.title('Training and Validation AUC Over Time')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# --------------- TEST SET EVALUATION ---------------\n",
    "\n",
    "# Load only the model's state_dict for testing\n",
    "checkpoint = torch.load(final_checkpoint_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print('Best model loaded for testing.')\n",
    "\n",
    "# Move test data to device\n",
    "test_data = test_data.to(device)\n",
    "test_labels_tp_s = test_labels_tp_s.to(device)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()  # Switch to evaluation mode\n",
    "with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "    test_out = model(test_data)\n",
    "    \n",
    "    # Extract source and target node embeddings for the tp_s test edges\n",
    "    test_source_embeddings = test_out[test_edges_tp_s[0]]\n",
    "    test_target_embeddings = test_out[test_edges_tp_s[1]]\n",
    "    \n",
    "    # Compute the edge scores (dot product or another method)\n",
    "    test_edge_scores = (test_source_embeddings * test_target_embeddings).sum(dim=1)\n",
    "    \n",
    "    # Compute test loss using edge scores and test labels\n",
    "    test_loss = criterion(test_edge_scores, test_labels_tp_s)\n",
    "    print(f'Test Loss: {test_loss.item()}')\n",
    "\n",
    "    # Test accuracy\n",
    "    test_predictions = torch.sigmoid(test_edge_scores) > 0.5\n",
    "    accuracy = (test_predictions == test_labels_tp_s).sum().item() / test_labels_tp_s.size(0)\n",
    "    print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "    # Test AUC\n",
    "    test_probs = torch.sigmoid(test_edge_scores).cpu().numpy()\n",
    "    test_labels_np = test_labels_tp_s.cpu().numpy()\n",
    "    test_auc = roc_auc_score(test_labels_np, test_probs)\n",
    "    print(f'Test AUC: {test_auc}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
